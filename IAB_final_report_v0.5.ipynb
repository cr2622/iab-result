{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20-S IAB term project**\n",
    "\n",
    "---\n",
    "# Spatiotemporal Demand Prediction Model for Personal Mobility using a Graph Convolutional Neural Network\n",
    "\n",
    "---\n",
    "## Authors\n",
    "\n",
    "**Seung-Woo Ham**\n",
    "\n",
    "- Github: [Github Profile](https://github.com/seungwooham)\n",
    "- email: [seungwoo.ham@snu.ac.kr](mailto:seungwoo.ham@snu.ac.kr)\n",
    "- For any question, feel free to contact me.\n",
    "\n",
    "**Jung-Hoon Cho**\n",
    "\n",
    "- Github: [Github Profile](https://github.com/cr2622)\n",
    "- Linkedin: [LinkedIn Profile](https://www.linkedin.com/in/junghoon-cho/)\n",
    "- email: [cr2622@snu.ac.kr](mailto:cr2622@snu.ac.kr)\n",
    "\n",
    "---\n",
    "## Data acquisition\n",
    "\n",
    "- Personal mobility data\n",
    "    * Deer Corporation([webpage](https://deering.co//))\n",
    "\n",
    "- Public transit usage data\n",
    "    * acquired in Seoul Metropolitan Government([http://data.seoul.go.kr/](http://data.seoul.go.kr/))\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- Transportation Laboratory, Seoul National University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* Introduction\n",
    "* Literature Review\n",
    " * Personal mobility\n",
    " * Demand prediction\n",
    "* Methodology\n",
    " * Graph Convolutional Network (GCN)\n",
    " * K-means clustering\n",
    " * others...\n",
    "* Empirical Analysis\n",
    " * Data description\n",
    " * Data preprocessing\n",
    " * Graph definition\n",
    "* Result and discussions\n",
    "* Conclusions\n",
    "* References\n",
    "* Acknowledgment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 우리나라에서 최근 주목 받고 있는 전동 킥보드 (Personal Mobility) 공유 서비스\n",
    " * 수요 예측과 재배치의 어려움\n",
    " * 고객의 주요 불만 사항 중 하나가 킥보드의 부재\n",
    " * 재배치 인력 배치에 많은 인건비가 소비됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정확한 시공간적 수요 예측을 통하여 서비스의 품질과 경제성 확보 가능\n",
    " * 시공간적 패턴을 반영하는 수요예측\n",
    " * Graph Convolutional Neural Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![background](01 figures/figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 전동 킥보드의 특성을 고려한 시공간적 수요 예측 알고리즘 개발\n",
    " * 실제 이용 기록/어플리케이션 사용 기록 등을 바탕으로 예측 정확도 개선\n",
    " * **Graph Convolutional Neural Network (GCN)** 적용\n",
    "* 시스템을 Graph로 정의하는 형태 필요\n",
    " * 이용 특성을 나타내는 graph가 더 정확한 수요예측에 기여할 수 있다.\n",
    " * 어떠한 형태가 더 적절한 그래프인지 확인 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체적인 연구의 흐름은 다음과 같다.\n",
    "우선 전체적인 연구의 배경과 연구의 목적을 설명하고, 관련된 선행 연구를 검토해보았다. 선행연구와 차별점이 어떤 것이 될 수 있을지 알아보고, 이 연구에서 사용할 방법론에 대해 설명한다. 이번 연구에서는 Graph Convolutional Network (GCN)과 K-means clustering 등 다양한 방법론이 사용된다. 이후 이어지는 Empirical Analysis에서는 사용할 데이터에 대한 소개와 데이터 전처리, GCN 예측 모델을 활용한 수요 예측과 그 예측력 평가와 분석이 이루어진다. 마지막 결론에서는 이번 연구의 결과에 대한 해석과 제언 등이 이루어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![research_process](01 figures/figure2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lin et al. (2018), Predicting station-level hourly demand in a large-scale bike-sharing network: A graph convolutional neural network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GCN with Data-driven Graph Filter (GCNN-DDGF) \n",
    " * regular GCNN-DDGF: convolution block and feedforward block\n",
    " * recurrent GCNN-DDGF: convolution block, feedforward block and recurrent block from LSTM\n",
    "* GCN with adjacency matrices \n",
    " * Spatial Distance matrix (SD), Demand matrix (DE), Average Trip Duration matrix (ATD), and Demand Correlation matrix (DC). \n",
    " * Evaluation Criteria : MAE, R^2\n",
    " * GCNN_rec−DDGF and GCNN_reg−DDGF perform the best on all criteria\n",
    "* Graph Network Analysis to understand \"black box“ of GCNN-DDGF\n",
    " * Weighted Degree (WD) : weighted sum of its edges\n",
    " * DDGF not only captures some of the same information existing in the SD, DE and DC matrices, but also uncovers hidden correlations among stations that are not revealed by any of these matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lin2018](01 figures/figure3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kim et al. (2019), Graph convolutional network approach applied to predict hourly bike-sharing demands considering spatial, temporal, and global effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GCN Framework\n",
    " * Repeated Temporal Pattern: hourly, daily and weekly data. -> concatenate -> FC Layer\n",
    " * Global Variables: weather, weekday/weekend\n",
    "* Adjacency Matrices\n",
    " * GCN-Inverse Distance Weight (GCN-IDW)\n",
    " * GCN-Usage Pattern (GCN-UP) have best performance\n",
    " * Evaluation Criteria : MSE, Average Pearson Correlation(APC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kim2019](01 figures/figure4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yang et al. (2019), A spatiotemporal and graph-based analysis of dockless bike sharing patterns to understand urban flows over the last mile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Graph- based analysis\n",
    " * Geostatistical analyses support understanding of large-scale changes in spatiotemporal travel behaviours and graph-based approaches allow changes in local travel flows between individual locations to be quantified and characterized.\n",
    " * The results show how the new metro service boosted nearby bike demand, but with considerable spatial variation, and changed the spatiotemporal patterns of bike travel behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![yang2019](01 figures/figure5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cho et al. (2020), Enhancing the Accuracy of Peak Hourly Demand in Bike-Sharing Systems using a Graph Convolutional Network with Public Transit Usage Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 연구 대상: 서울시 공공자전거 따릉이\n",
    "* 연구 범위: 영등포구 여의도동 (Figure 2)\n",
    "* 기존의 demand correlation 분석해보면 각 정류장 간 대여특성이 보인다. (Figure 4)\n",
    "* GCN 활용하면 Accuracy 높아짐. (Figure 3)\n",
    "* 버스 승하차 데이터를 넣었을 때 유의미한 차이 보이며, 해당 지역의 특성 반영하는 것처럼 보임. (Figure 5). \n",
    "* 특히 peak 예측능력 개선이 유의미함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cho2020](01 figures/figure6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ham et al. (2020), Spatiotemporal Demand Prediction Model for Personal Mobility with Latent Feature and Deep Learning: Considering its Intrinsic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 연구 대상: 공유 전동 킥보드 DEER\n",
    "* 연구 범위: 성동구, 광진구 일부 지역\n",
    "* Encoder-RNN-Decoder (ERD) 도입\n",
    "* Latent feature를 추출하는데 효과적."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ham2020](01 figures/figure7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Neural Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* consider relationship among stations that can be represented in a graphical structure\n",
    "* 그래프의 형태로 표현할 수 있는 데이터에 적합\n",
    "* Image는 CNN 이용, Time series 데이터는 RNN 이용\n",
    "* 공간 정보를 node와 edge로 표현할 수 있다면 GCN 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple architecture of CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simplecnn](01 figures/figure8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple two-layer GCN\n",
    " * First introduced in Kipf et al. (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simplegcn](01 figures/figure9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1~4의 node와 이를 잇는 edge로 이루어진 graph\n",
    "* 각 node는 feature를 갖고 있음 (1의 feature는 [1 0 0])\n",
    "* 연결성을 나타내는 A와 각 node의 feature를 나타내는 X를 구성 가능!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](01 figures/figure10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 feature의 어느 정도의  가중치를 줄 지 결정하는 weight matrix를 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![adjandweightmatrix](01 figures/figure11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [4×4] ×[4×3] ×[3×8] = [4×8] =  X'\n",
    "* 길이 8의 latent feature vector가 나타남\n",
    "* Adjacency matrix에 의해 서로 연결되어 있는 node만 영향을 미침\n",
    "* 결과로 나타난 X'은 non-linear activation에 넣음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Architecture of GCN hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gcnhiddenlayer](01 figures/figure12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* STGCN Framework (Yu et al. (2018), Cho et al. (2020))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stgcn](01 figures/figure13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GCN with hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prediction Methodologies\n",
    " * Historical Average\n",
    " * the historical Average (HA) calculates the predicting demand of the next hour on the average of historical demands for the previous five hours at each station\n",
    "\n",
    "\n",
    "* Long Short-Term Memory (LSTM)\n",
    " * a special kind of RNN, capable of learning long-term dependencies\n",
    " * explicitly designed to avoid the long-term dependency problem\n",
    "![lstm](01 figures/figure14.png)\n",
    "\n",
    "* GCN\n",
    " * With different graph structures\n",
    " * GRID/Cluster/Subway/Bus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to determine node?\n",
    " * Grid structure\n",
    " * 14X18 grid (*Walkable distance)\n",
    "* Geographical Cluster\n",
    " * K-means clustering\n",
    " * K = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 30, 40, 50, 75, 100, 150, 200, 250, 252]\n",
    "* Subway station cluster\n",
    " * 12 stations\n",
    "* Bus station cluster\n",
    " * 204 stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph construction](01 figures/figure15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluation Criteria\n",
    " * RMSE and MAE\n",
    "\t![rmse,mae](01 figures/figure16.png)\n",
    "\tWhere, 𝑀 is the number of hourly time steps, 𝑁 is the number of stations, 𝑦_𝑖𝑗 and 𝑦 ̂_𝑖𝑗 stand for actual and predicted demand at hourly time step 𝑖 and station 𝑗, respectively.\n",
    "* Hyperparameter\n",
    " * Grid search\n",
    " * Find optimal value\n",
    " * In this study, we use\n",
    "\tOptimizer = Adam, epochs = 200, n_layers = 4, dropout = False, lr = 0.001, step_size = 150, gamma = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Empirical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Personal mobility transaction data (DEER Corporation)\n",
    "* Rent time, rent location, return time, return location, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time scope**\n",
    "* Total dataset: September 8th , 2019 ~ October 10th , 2019\n",
    "    * Train set: 2019-09-08 00:00:00~2019-09-28 19:00:00 (500 hrs)\n",
    "    * Test set: 2019-09-28 20:00:00~2019-10-11 00:00:00 (292 hrs)\n",
    "* For every hour, with 5 hours backward, predicting 1 hour forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geographical scope**\n",
    "* Seongdong-gu and Gwanjin-gu, Seoul\n",
    "* Near Konkuk University Station, \n",
    "* has both a shopping district and residences. \n",
    "* 12 subway stations, and 330 bus stations (aggregated to 204) are located within the service area\n",
    "* one of which is outside of the service area but is located nearby. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![studyarea](01 figures/figure18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descriptive Statistics of the Dataset**\n",
    "\n",
    "*전동킥보드 대여 특성*\n",
    "\n",
    "* Unmet demand의 비율이 아주 높다.\n",
    " * 빌리고 싶었지만 빌리지 못하는 비율이 높았음.\n",
    " * 전체적인 자전거 수의 부족 문제도 있지만 공간적 불균형 문제가 컸다.\n",
    "* 15분 내외의 이동이 대다수임.\n",
    " * 공공자전거 평균 대여시간 36.3분\n",
    " * 서울시 공공자전거에 비해 목적통행보다 수단통행의 비율이 높다고 추정할 수 있음.\n",
    " * 이를 참고하여 grid size 결정\n",
    "* 평균 이동거리는 약 1.231 km\n",
    " * 서울시 공공자전거의 평균 이동 거리 4,272 km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![escooter](01 figures/figure17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid 크기 설정을 위한 접근 시간 계산**\n",
    "\n",
    "* 해당 전동 킥보드의 영업 범위에서 전동 킥보드 사용을 15분 이상 걷는 것은 적절한 사용이라 보기 어려움\n",
    "* Walkfore 지점부터, 이용 지점까지의 거리, 남은 시간으로 속도 확인하여 도보 이용만 골라냄\n",
    "* 5m/s 이하의 속도로 접근중이면서, 15분내의 walkfore와 가장 최근 이용을 matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grid](01 figures/figure19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**접근시간 산정 코드**\n",
    "* 어플리케이션 데이터와 매칭된 table을 활용하여 이용까지 걸리는 평균 시간 산정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cal_grid_size:\n",
    "    def __init__(self, filename):\n",
    "        self.raw_data = pd.read_csv(filename, header=0)\n",
    "        \n",
    "        distance_list = []\n",
    "        \n",
    "        for i in range(len(self.raw_data)):\n",
    "            distance_tmp = geodesic((self.raw_data['walk_fore_lat'][i], self.raw_data['walk_fore_lng'][i]),\n",
    "                                    (self.raw_data['use_start_lat'][i], self.raw_data['use_start_lng'][i])).meters\n",
    "            distance_list.append(distance_tmp)\n",
    "        \n",
    "        self.raw_data['distance'] = distance_list\n",
    "        \n",
    "    def get_time_diff(self):\n",
    "        self.raw_data['walk_fore_at'] = pd.to_datetime(self.raw_data['walk_fore_at'])\n",
    "        self.raw_data['use_start_at'] = pd.to_datetime(self.raw_data['use_start_at'])\n",
    "\n",
    "        self.raw_data['eta'] = self.raw_data['use_start_at'] - self.raw_data['walk_fore_at']\n",
    "        self.raw_data['eta'] = self.raw_data['eta'].dt.total_seconds()\n",
    "        self.raw_data['speed'] = self.raw_data['distance']/self.raw_data['eta']\n",
    "\n",
    "        self.speed_limit = self.raw_data.loc[self.raw_data['speed']<5]\n",
    "        self.speed_limit = self.speed_limit.drop_duplicates(['use_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_grid_tb = cal_grid_size('01_fore_use.csv')\n",
    "cal_grid_tb.get_time_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (\n",
    "    ggplot(cal_grid_tb.speed_limit[['eta', 'distance']], aes(x='eta', y='distance')) + \n",
    "    geom_point(size=0.01) + geom_smooth(span=1, color='darkred', size=1.5) + geom_rug(size=0.01) + \n",
    "    xlab('Time to Start Usage (sec)') + ylab('Distance to E-Scooter (m)') + \n",
    "    ggtitle('Distance to E-Scooter by Time to Start Usage') + \n",
    "    theme(\n",
    "        axis_text_x=element_text(size=10), axis_title_x=element_text(size=13),\n",
    "        axis_text_y=element_text(size=10), axis_title_y=element_text(size=13),\n",
    "        title=element_text(size=15)\n",
    "    )\n",
    ")\n",
    "\n",
    "plot\n",
    "\n",
    "# Uncomment the line below to save the figure\n",
    "# ggsave(plot, 'Distance to E-Scooter by Time to Start Usage.png', dpi=700)\n",
    "\n",
    "\n",
    "# Uncomment the line below to save dataframe as csv\n",
    "# cal_grid_tb.speed_limit.to_csv('02_speed_dup_del.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unmet Demand 확인 코드**\n",
    "* Map zoom table에는 지도를 확대하는 행위들이 담겨있고 column은 각각 'map zoom', 'map_zoom_id', 'map_zoom_user_id', 'map_zoom_lat', 'map_zoom_lng', 'map_zoom_level', 'map_zoom_at'임\n",
    "* Walk fore table에는 어플리케이션 기록이 있으며 column은 각각 'walk_fore_id', 'walk_fore_user_id', 'walk_fore_lat', 'walk_fore_lng', 'walk_fore_at'임\n",
    "* Table을 'user id'와 'time'에 대해 정렬, 이후 map zoom table과 walk fore table을 left join함\n",
    "* Left join된 table은 다시 use table과 비교되어 unmet demand 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_zoom = pd.read_csv('03_map_zoom_tb.csv')\n",
    "walk_fore = pd.read_csv('04_walk_fore_tb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_zoom = map_zoom.sort_values(by=['map_zoom_user_id', 'map_zoom_at'])\n",
    "map_zoom = map_zoom.reset_index(drop=True)\n",
    "map_zoom['map_zoom_at'] = pd.to_datetime(map_zoom['map_zoom_at'])\n",
    "map_zoom['map_zoom_at_round'] = map_zoom.map_zoom_at.dt.round('5min')\n",
    "\n",
    "walk_fore = walk_fore.sort_values(by=['walk_fore_user_id', 'walk_fore_at'])\n",
    "walk_fore = walk_fore.reset_index(drop=True)\n",
    "walk_fore['walk_fore_at'] = pd.to_datetime(walk_fore['walk_fore_at'])\n",
    "walk_fore['walk_fore_at_round'] = walk_fore.walk_fore_at.dt.round('5min')\n",
    "walk_fore = walk_fore.drop_duplicates(subset=['walk_fore_user_id', 'walk_fore_at_round'], keep='last')\n",
    "walk_fore = walk_fore.reset_index(drop=True)\n",
    "\n",
    "zoom_fore = pd.merge(map_zoom, walk_fore, how='left', \n",
    "                     left_on=['map_zoom_user_id', 'map_zoom_at_round'], \n",
    "                     right_on=['walk_fore_user_id', 'walk_fore_at_round'])\n",
    "\n",
    "zoom_fore = zoom_fore[np.isfinite(zoom_fore['walk_fore_lng'])]\n",
    "zoom_fore = zoom_fore.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_list = []\n",
    "\n",
    "for i in range(len(zoom_fore)):\n",
    "    distance_tmp = geodesic((zoom_fore['map_zoom_lat'][i], zoom_fore['map_zoom_lng'][i]), \n",
    "                            (zoom_fore['walk_fore_lat'][i], zoom_fore['walk_fore_lng'][i])).meters\n",
    "    distance_list.append(distance_tmp)\n",
    "\n",
    "zoom_fore['distance'] = distance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_fore = zoom_fore.sort_values(by=['walk_fore_user_id', 'walk_fore_at_round', 'distance'])\n",
    "zoom_fore = zoom_fore.drop_duplicates(subset=['walk_fore_user_id', 'walk_fore_at_round'], keep='first')\n",
    "zoom_fore = zoom_fore[zoom_fore['distance']<251]\n",
    "zoom_fore = zoom_fore.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_use = pd.read_csv('01_fore_use.csv')\n",
    "fore_use.drop_duplicates(subset=['use_id'], keep='first', inplace=True)\n",
    "missing_call = zoom_fore[~zoom_fore['walk_fore_id'].isin(list(set(fore_use['walk_fore_id']) & set(zoom_fore['walk_fore_id'])))]\n",
    "\n",
    "# Uncomment the line below to save dataframe as csv\n",
    "# missing_call.to_csv('06_missing_call.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**중복 Row 처리**\n",
    "* missing call과 od_mat 사이의 중복 처리를 위한 od_mat 내부 새로운 column 생성\n",
    "* left only를 활용하여 다시 한 번 missing call 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_mat['use_start_at_round_15min'] = od_mat['use_start_at'].dt.round('15min')\n",
    "miss_o['walk_fore_at_round_15min'] = miss_o['walk_fore_at'].dt.round('15min')\n",
    "miss_o = miss_o.drop_duplicates(['walk_fore_user_id', 'walk_fore_at_round_15min'], keep='first').reset_index(drop=True)\n",
    "\n",
    "miss_o_trim = pd.merge(miss_o, od_mat, left_on=['walk_fore_user_id', 'walk_fore_at_round_15min'], right_on=['use_user_id', 'use_start_at_round_15min'], how='outer', indicator=True)\n",
    "miss_o_trim = miss_o_trim[miss_o_trim['_merge'] == 'left_only']\n",
    "miss_o_trim = miss_o_trim[miss_o_trim.columns[:8]]\n",
    "miss_o_trim = miss_o_trim.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**거리에 따른 triming**\n",
    "* 분석 범위의 설정\n",
    "* 분석 범위 외의 데이터는 삭제\n",
    "* lat_start = 37.526531\n",
    "* lng_start = 127.037807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_by_range(data1, data2, start_cor, end_cor):\n",
    "    data1 = data1[(data1['use_start_x']>start_cor[0]) & (data1['use_start_y']>start_cor[1]) &\n",
    "                  (data1['use_end_x']>start_cor[0]) & (data1['use_end_y']>start_cor[1]) &\n",
    "                  (data1['use_start_x']<end_cor[0]) & (data1['use_start_y']<end_cor[1]) &\n",
    "                  (data1['use_end_x']<end_cor[0]) & (data1['use_end_y']<end_cor[1])]\n",
    "    \n",
    "    data2 = data2[(data2['walk_fore_x']>start_cor[0]) & (data2['walk_fore_y']>start_cor[1]) &\n",
    "                  (data2['walk_fore_x']<end_cor[0]) & (data2['walk_fore_y']<end_cor[1])]\n",
    "\n",
    "    data1 = data1.reset_index(drop=True)\n",
    "    data2 = data2.reset_index(drop=True)\n",
    "    \n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [500, 500]\n",
    "end = [5000, 4000]\n",
    "od_mat, miss_o_trim = cut_by_range(od_mat, miss_o_trim, start, end)\n",
    "# However, it is already done in the previous stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-means clustering**\n",
    "- based on geographical location(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = list(range(1,21,1))+list(range(30,51,10))+[75, 100, 150, 200, 250, 252]\n",
    "\n",
    "for k in k_list:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(np.vstack((train_od_mat[['use_start_x', 'use_start_y']].values, train_miss_o[['walk_fore_x', 'walk_fore_y']].values)))\n",
    "    train_cluster = kmeans.predict(np.vstack((train_od_mat[['use_start_x', 'use_start_y']].values, train_miss_o[['walk_fore_x', 'walk_fore_y']].values)))\n",
    "    test_od_mat_cluster = kmeans.predict(test_od_mat[['use_start_x', 'use_start_y']])\n",
    "    test_miss_o_cluster = kmeans.predict(test_miss_o[['walk_fore_x', 'walk_fore_y']])\n",
    "    \n",
    "    train_od_mat[str(k)+' clusters'] = pd.Series(train_cluster[:18674])\n",
    "    train_miss_o[str(k)+' clusters'] = pd.Series(train_cluster[18674:])\n",
    "    test_od_mat[str(k)+' clusters'] = pd.Series(test_od_mat_cluster)\n",
    "    test_miss_o[str(k)+' clusters'] = pd.Series(test_miss_o_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = True # Change to false to write\n",
    "\n",
    "if read == False:\n",
    "    with open('train_od_mat_18674.p', 'wb') as f:\n",
    "        pickle.dump(train_od_mat, f)\n",
    "    with open('train_miss_o_16541.p', 'wb') as f:\n",
    "        pickle.dump(train_miss_o, f)        \n",
    "    with open('test_od_mat_11342.p', 'wb') as f:\n",
    "        pickle.dump(test_od_mat, f)\n",
    "    with open('test_miss_o_11367.p', 'wb') as f:\n",
    "        pickle.dump(test_miss_o, f)\n",
    "\n",
    "else:\n",
    "    with open('train_od_mat_18674.p', 'rb') as f:\n",
    "        train_od_mat = pickle.load(f)\n",
    "    with open('train_miss_o_16541.p', 'rb') as f:\n",
    "        train_miss_o = pickle.load(f)\n",
    "    with open('test_od_mat_11342.p', 'rb') as f:\n",
    "        test_od_mat = pickle.load(f)\n",
    "    with open('test_miss_o_11367.p', 'rb') as f:\n",
    "        test_miss_o = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Pre-processing with K-means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_demand_array(data, start, end, k, od_mat):\n",
    "    date_time_range = pd.date_range(start=start, end=end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "    time_demand = np.zeros((k, len(date_time_range)))\n",
    "    \n",
    "    if od_mat==True:\n",
    "        data_k = data[['use_user_id', 'use_start_at', 'use_start_x', 'use_start_y', 'use_end_at', 'use_end_x', 'use_end_y',\n",
    "                       'use_start_at_round', 'use_end_at_round', str(k)+' clusters']]\n",
    "        for idx, date_time in enumerate(date_time_range):\n",
    "            data_k_date_time = data_k[data_k['use_start_at_round']==date_time]\n",
    "            for cluster in range(k):\n",
    "                time_demand[cluster][idx] += (data_k_date_time[str(k)+' clusters']==cluster).sum()\n",
    "    else:\n",
    "        data_k = data[['walk_fore_user_id', 'walk_fore_at', 'walk_fore_x', 'walk_fore_y', 'walk_fore_at_round', str(k)+' clusters']]\n",
    "        for idx, date_time in enumerate(date_time_range):\n",
    "            data_k_date_time = data_k[data_k['walk_fore_at_round']==date_time]\n",
    "            for cluster in range(k):\n",
    "                time_demand[cluster][idx] += (data_k_date_time[str(k)+' clusters']==cluster).sum()\n",
    "                \n",
    "    return time_demand\n",
    "\n",
    "def get_normalized_adj(A):\n",
    "    # A = A + np.diag(np.ones(A.shape[0], dtype=np.float32))\n",
    "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
    "    D[D <= 10e-5] = 10e-5\n",
    "    diag = np.reciprocal(np.sqrt(D))\n",
    "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A), diag.reshape((1, -1)))\n",
    "    return A_wave\n",
    "\n",
    "def create_a(train_od_mat, train_miss_o, k):\n",
    "    avg_x_y = []\n",
    "    train_od_mat_k = train_od_mat[['use_user_id', 'use_start_at', 'use_start_x', 'use_start_y',\n",
    "                                   'use_end_at', 'use_end_x', 'use_end_y', 'use_start_at_round', 'use_end_at_round',\n",
    "                                   str(k)+' clusters']]  \n",
    "    train_miss_o_k = train_miss_o[['walk_fore_user_id', 'walk_fore_at',\n",
    "                                   'walk_fore_x', 'walk_fore_y', 'walk_fore_at_round', str(k)+' clusters']]\n",
    "    for cluster in range(k):\n",
    "        train_od_mat_len = len(train_od_mat[train_od_mat[str(k)+' clusters']==cluster])\n",
    "        train_od_mat_sum_x = train_od_mat[train_od_mat[str(k)+' clusters']==cluster]['use_start_x'].sum()\n",
    "        train_od_mat_sum_y = train_od_mat[train_od_mat[str(k)+' clusters']==cluster]['use_start_y'].sum()\n",
    "\n",
    "        train_miss_o_len = len(train_miss_o[train_miss_o[str(k)+' clusters']==cluster])\n",
    "        train_miss_o_sum_x = train_miss_o[train_miss_o[str(k)+' clusters']==cluster]['walk_fore_x'].sum()\n",
    "        train_miss_o_sum_y = train_miss_o[train_miss_o[str(k)+' clusters']==cluster]['walk_fore_y'].sum()\n",
    "\n",
    "        avg_x = (train_od_mat_sum_x+train_miss_o_sum_x)/(train_od_mat_len+train_miss_o_len)\n",
    "        avg_y = (train_od_mat_sum_y+train_miss_o_sum_y)/(train_od_mat_len+train_miss_o_len)\n",
    "        avg_x_y.append([avg_x, avg_y])\n",
    "    \n",
    "    dist_mat = [[0 for x in range(k)] for y in range(k)] \n",
    "\n",
    "    for i, cluster in enumerate(range(k)):\n",
    "        for j, cluster in enumerate(range(i,k)):\n",
    "            dist_mat[i][k-1-j] = ((avg_x_y[i][0]-avg_x_y[k-1-j][0])**2+(avg_x_y[i][1]-avg_x_y[k-1-j][1])**2)**0.5\n",
    "            dist_mat[k-1-j][i] = dist_mat[i][k-1-j]\n",
    "    \n",
    "    dist_mat = np.asarray(dist_mat)\n",
    "    print('Average distance between stations are {} m'.format(dist_mat.mean()))\n",
    "    \n",
    "    distance_threshold = True\n",
    "    \n",
    "    if distance_threshold == False:\n",
    "        '''\n",
    "        Create matrix with 1 and 0 by distance threshold\n",
    "        '''\n",
    "        dist_mat[np.where(dist_mat <= dist_mat.mean()/5)] = 1\n",
    "        dist_mat[np.where(dist_mat > dist_mat.mean()/5)] = 0\n",
    "\n",
    "        print('From total {}*{}={} combinations, {} pairs are selected it is near each other'.format(k, k, k**2, dist_mat.sum()))\n",
    "        A_mat = torch.FloatTensor(get_normalized_adj(dist_mat))\n",
    "    \n",
    "    else:\n",
    "        '''\n",
    "        Create matrix with reciprocal of distance\n",
    "        '''\n",
    "        for i in range(len(dist_mat)):\n",
    "            dist_mat[i][i] = dist_mat[i].min()\n",
    "        A_mat = torch.FloatTensor(get_normalized_adj(dist_mat))\n",
    "        \n",
    "    return A_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 0 : Grid structure**\n",
    "* Baseline\n",
    "* num_cluster = 14*18=252\n",
    "* Grid Centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grid](01 figures/figure20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 1 : Geographical Cluster**\n",
    "* K-means clustering\n",
    "* 생성된 Train/Test data에 cluster 번호 등록 중\n",
    "* 대략 15개 근방에서 elbow를 확인함\n",
    "* [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 30, 40, 50, 75, 100, 150, 200, 250, 252]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![geographicalcluster](01 figures/figure21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 2 : Subway station Cluster**\n",
    "* num_cluster = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![subway](01 figures/figure22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 3 : Bus station Cluster**\n",
    "* num_cluster = 204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bus](01 figures/figure23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency matrix and Node feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjacency matrix**\n",
    "* Distance matrix\n",
    "    * 설정된 노드 간의 geographical distance를 활용\n",
    "    * A_wave 만들기 위해 get_normalized_adj() 함수 정의\n",
    "\n",
    "\n",
    "**Node feature matrix**\n",
    "* Weather\n",
    " * 기상청 시간대별 자료 활용\n",
    " * temperature, wind, rainfall\n",
    "* Holiday\n",
    " * 그날이 평일인지, 주말인지 공휴일인지를 나타내는 binary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Average (HA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = list(range(1,21,1))+list(range(30,51,10))+[75, 100, 150, 200, 250, 252]\n",
    "\n",
    "ha = {}\n",
    "\n",
    "for k in k_list:\n",
    "    # print(test_all_demand[k].shape)\n",
    "    time_len = len(test_all_demand[k][0])\n",
    "    time_slice = np.zeros((time_len-5, k))\n",
    "\n",
    "    for idx in range(time_len-5):\n",
    "        ha_time_slice = np.zeros((k))\n",
    "        for c in range(k):\n",
    "            ha_k = test_all_demand[k][c,idx:idx+5].mean()\n",
    "            ha_time_slice[c] = ha_k\n",
    "#         print(ha_time_slice)\n",
    "        time_slice[idx] = ha_time_slice\n",
    "    ha[k] = time_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_HA = {}\n",
    "for k in k_list:\n",
    "    MSE_HA[k] = mean_squared_error(ha[k],test_all_y_dic[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_HA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN - k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2019-09-08 00:00:00'\n",
    "train_end = '2019-09-28 19:00:00'\n",
    "test_start = '2019-09-28 20:00:00'\n",
    "test_end = '2019-10-11 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "\n",
    "# Data with clusters\n",
    "with open('train_od_mat_18674.p', 'rb') as f:\n",
    "    train_od_mat = pickle.load(f)\n",
    "with open('train_miss_o_16541.p', 'rb') as f:\n",
    "    train_miss_o = pickle.load(f)\n",
    "with open('test_od_mat_11342.p', 'rb') as f:\n",
    "    test_od_mat = pickle.load(f)\n",
    "with open('test_miss_o_11367.p', 'rb') as f:\n",
    "    test_miss_o = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "with open('train_ds_all.p', 'rb') as f:\n",
    "    train_ds_all = pickle.load(f)\n",
    "with open('train_loader_all.p', 'rb') as f:\n",
    "    train_loader_all = pickle.load(f)\n",
    "with open('test_ds_all.p', 'rb') as f:\n",
    "    test_ds_all = pickle.load(f)\n",
    "with open('test_loader_all.p', 'rb') as f:\n",
    "    test_loader_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**필요한 함수 정의하기**\n",
    "* get_normalized_adj() : A_wave를 만들기 위한 함수\n",
    "* create_a() : 다양한 input data(distance)를 adjacency matrix의 형태로 만들기 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_adj(A):\n",
    "    A = A + np.diag(np.ones(A.shape[0], dtype=np.float32))\n",
    "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
    "    D[D <= 10e-5] = 10e-5\n",
    "    diag = np.reciprocal(np.sqrt(D))\n",
    "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A), diag.reshape((1, -1)))\n",
    "    return A_wave\n",
    "\n",
    "def create_a(train_od_mat, train_miss_o, k):\n",
    "    avg_x_y = []\n",
    "    train_od_mat_k = train_od_mat[['use_user_id', 'use_start_at', 'use_start_x', 'use_start_y',\n",
    "                                   'use_end_at', 'use_end_x', 'use_end_y', 'use_start_at_round', 'use_end_at_round',\n",
    "                                   str(k)+' clusters']]  \n",
    "    train_miss_o_k = train_miss_o[['walk_fore_user_id', 'walk_fore_at',\n",
    "                                   'walk_fore_x', 'walk_fore_y', 'walk_fore_at_round', str(k)+' clusters']]\n",
    "    for cluster in range(k):\n",
    "        train_od_mat_len = len(train_od_mat[train_od_mat[str(k)+' clusters']==cluster])\n",
    "        train_od_mat_sum_x = train_od_mat[train_od_mat[str(k)+' clusters']==cluster]['use_start_x'].sum()\n",
    "        train_od_mat_sum_y = train_od_mat[train_od_mat[str(k)+' clusters']==cluster]['use_start_y'].sum()\n",
    "\n",
    "        train_miss_o_len = len(train_miss_o[train_miss_o[str(k)+' clusters']==cluster])\n",
    "        train_miss_o_sum_x = train_miss_o[train_miss_o[str(k)+' clusters']==cluster]['walk_fore_x'].sum()\n",
    "        train_miss_o_sum_y = train_miss_o[train_miss_o[str(k)+' clusters']==cluster]['walk_fore_y'].sum()\n",
    "\n",
    "        avg_x = (train_od_mat_sum_x+train_miss_o_sum_x)/(train_od_mat_len+train_miss_o_len)\n",
    "        avg_y = (train_od_mat_sum_y+train_miss_o_sum_y)/(train_od_mat_len+train_miss_o_len)\n",
    "        avg_x_y.append([avg_x, avg_y])\n",
    "    \n",
    "    dist_mat = [[0 for x in range(k)] for y in range(k)] \n",
    "\n",
    "    for i, cluster in enumerate(range(k)):\n",
    "        for j, cluster in enumerate(range(i,k)):\n",
    "            dist_mat[i][k-1-j] = (((avg_x_y[i][0]-avg_x_y[k-1-j][0])**2+(avg_x_y[i][1]-avg_x_y[k-1-j][1])**2)**0.5)/1000.\n",
    "            dist_mat[k-1-j][i] = dist_mat[i][k-1-j]\n",
    "    \n",
    "    dist_mat = np.asarray(dist_mat)\n",
    "    print('For {} clusters, average distance between stations are {} km'.format(k, dist_mat.mean()))\n",
    "    \n",
    "    distance_threshold = False\n",
    "    \n",
    "    if distance_threshold == True:\n",
    "        '''\n",
    "        Create matrix with 1 and 0 by distance threshold\n",
    "        '''\n",
    "        print(dist_mat)\n",
    "        dist_mat[np.where(dist_mat <= dist_mat.mean()/1.5)] = 1\n",
    "        dist_mat[np.where(dist_mat > dist_mat.mean()/1.5)] = 0\n",
    "        \n",
    "        print('From total {}*{}={} combinations, {} pairs are selected it is near each other'.format(k, k, k**2, dist_mat.sum()))\n",
    "        print(dist_mat)\n",
    "        A_mat = torch.FloatTensor(get_normalized_adj(dist_mat))\n",
    "    \n",
    "    else:\n",
    "        '''\n",
    "        Create matrix with reciprocal of distance\n",
    "        '''\n",
    "        A_mat = torch.FloatTensor(get_normalized_adj(dist_mat))\n",
    "        \n",
    "    return A_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 cluster에 대해서 A_mat를 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mat = {}\n",
    "\n",
    "k_list = list(range(1,21,1))+list(range(30,51,10))+[75, 100, 150, 200, 250, 252]\n",
    "\n",
    "for k in k_list:\n",
    "    A_mat[k] = create_a(train_od_mat, train_miss_o, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN class 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.0, use_activation=True, bias=False,\n",
    "                 use_skip_connection=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_activation = use_activation\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        # PyTorch initialization (see docs)\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, X):\n",
    "        node_features, adj_matrix = X\n",
    "        \n",
    "        x = torch.einsum('ijk,km->ijm', node_features.cpu(), self.weight.cpu()).cuda()\n",
    "        x = torch.einsum('ii,jik->jik', adj_matrix.cpu(), x.cpu()).cuda()\n",
    "\n",
    "        # Add bias if necessary\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "\n",
    "        # Apply activation\n",
    "        if self.use_activation:\n",
    "            x = F.relu(x)\n",
    "\n",
    "        if self.use_skip_connection:\n",
    "            x += node_features\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return [x, adj_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gcn(nn.Module):\n",
    "    def __init__(self, nfeatures, nclasses, hidden_size=16, nhidden_layers=0, dropout=0.0,\n",
    "                 use_skip_connection=False):\n",
    "        super(Gcn, self).__init__()\n",
    "\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nclasses = nclasses\n",
    "        self.nfeatures = nfeatures\n",
    "        self.nhidden_layers = nhidden_layers\n",
    "\n",
    "        self.input_layer = GraphConvolution(nfeatures, hidden_size, self.dropout)\n",
    "\n",
    "        # do not use dropout in hidden layers\n",
    "        self.hidden_layers = self.create_conv_sequence(self.nhidden_layers, self.hidden_size,\n",
    "                                                       use_skip_connection=self.use_skip_connection)\n",
    "        self.output_layer = GraphConvolution(hidden_size, nclasses, self.dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_conv_sequence(num_of_layers, hidden_size, use_skip_connection=False):\n",
    "        seq_conv = nn.Sequential()\n",
    "        for i in range(num_of_layers):\n",
    "            seq_conv.add_module(\"Conv \" + str(i),\n",
    "                                GraphConvolution(hidden_size,\n",
    "                                                 hidden_size,\n",
    "                                                 use_skip_connection=use_skip_connection))\n",
    "        return seq_conv\n",
    "\n",
    "    def forward(self, features, adj_matrix):\n",
    "        x = self.input_layer([features, adj_matrix])\n",
    "        x = self.hidden_layers(x)\n",
    "        output = self.output_layer(x)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train process가 이루어지도록 하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, A_mat, loader_all, optimizer, epochs, k, n_layers, dropout, lr, step_size, gamma):\n",
    "\n",
    "    model.train()\n",
    "    loss = nn.MSELoss()\n",
    "    \n",
    "    train_history = {\"train_loss\": []}\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        for train_x_data, train_y_data in loader_all:\n",
    "            train_x_data.cuda()\n",
    "            train_y_data.cuda()\n",
    "            output = model(train_x_data, A_mat)\n",
    "            output = output.squeeze()\n",
    "            loss_train = loss(output.cuda(), train_y_data.cuda()) \n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "        # save current loss\n",
    "        scheduler.step()\n",
    "        train_history[\"train_loss\"].append(loss_train.item())\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(\"epochs: {}, loss: {}\".format(i, loss_train))\n",
    "        \n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test process가 이루어지도록 하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, A_mat, loader_all, k):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        test_history = {\"test_loss\": []}\n",
    "\n",
    "        for test_x_data, test_y_data in loader_all:\n",
    "            test_x_data.cuda()\n",
    "            test_y_data.cuda()\n",
    "            output = model(test_x_data, A_mat)\n",
    "            output = output.squeeze()\n",
    "            loss_test = loss(output.cuda(), test_y_data.cuda())\n",
    "        # save current loss\n",
    "        test_history[\"test_loss\"].append(loss_test.item())\n",
    "\n",
    "    return test_history, output, test_y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = list(range(1,21,1))+list(range(30,51,10))+[75, 100, 150, 200, 250, 252]\n",
    "# k_list = [2, 4, 6, 8, 10, 15, 20, 30, 50, 75, 100, 150, 200, 252]\n",
    "# k_list = [20]\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 32\n",
    "dropout = 0.0\n",
    "\n",
    "n_layers = 4\n",
    "n_iteration = 10\n",
    "\n",
    "# Train parameters (Adam optimizer is used)\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "step_size = 150\n",
    "gamma = 0.2\n",
    "\n",
    "# setting filename\n",
    "plt_date = \"0729 00 gcn-cluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCN formulating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'/04_result_analysis/')\n",
    "\n",
    "# Without skip connection\n",
    "\n",
    "wo_skip_train_history = []\n",
    "wo_skip_test_results = []\n",
    "\n",
    "for k in k_list:\n",
    "    for iteration in range(n_iteration):\n",
    "        print(\"==============================\")\n",
    "        print(\"number of cluster:\", k)\n",
    "        print(\"number of iteration:\", iteration)\n",
    "        model = Gcn(nfeatures=33, \n",
    "                    nhidden_layers=n_layers, \n",
    "                    hidden_size= hidden_size,\n",
    "                    nclasses= 1,\n",
    "                    dropout=dropout,\n",
    "                    use_skip_connection=False)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            A_mat[k] = A_mat[k].cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        plt_name = \" k=\"+str(k)+\" \"\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "        train_losses = train(model, A_mat[k], train_loader_all[k], optimizer, epochs, k, n_layers, dropout, lr, step_size, gamma)\n",
    "        \n",
    "        matplotlib.rcdefaults()\n",
    "        plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "        plt.rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('training and validation loss'+plt_name)\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim(0, 40)\n",
    "        plt.plot(train_losses['train_loss'], label='Train History')\n",
    "        plt.legend()\n",
    "        plt.savefig(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\".png\", quality=100, optimize=True, progressive=True)\n",
    "        \n",
    "        test_losses, test_pred, test_true = test(model, A_mat[k], test_loader_all[k], k)\n",
    "        \n",
    "        np.savetxt(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\"_pred.csv\", test_pred.detach().cpu().numpy().reshape(test_pred.shape[0:2]), delimiter=\",\")\n",
    "        np.savetxt(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\"_true.csv\", test_true.detach().cpu().numpy().reshape(test_true.shape[0:2]), delimiter=\",\")\n",
    "\n",
    "        print(\"Training, Test loss: ({:.6f}, {:.6f})\".format(train_losses['train_loss'][-1], test_losses['test_loss'][-1]))\n",
    "\n",
    "        wo_skip_train_history.append(train_losses)\n",
    "        wo_skip_test_results.append(test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN - Subway and Bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation for subway**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2019-09-08 00:00:00'\n",
    "train_end = '2019-09-28 19:00:00'\n",
    "test_start = '2019-09-28 20:00:00'\n",
    "test_end = '2019-10-11 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw/subway_and_bus_station')\n",
    "subway_station = pd.read_csv('subway_station.csv')\n",
    "start_lat = 37.526531\n",
    "start_lng = 127.037807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=subway_station\n",
    "data = data[(data['substn_lat']>start_lat) & (data['substn_lon']>start_lng) & (data['substn_lat']>start_lat) & (data['substn_lon']>start_lng)]\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "    \n",
    "start_coordinate_x = []\n",
    "start_coordinate_y = []\n",
    "end_coordinate_x = []\n",
    "end_coordinate_y = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    start_coordinate_x_tmp = geodesic((start_lat, start_lng), (start_lat, data['substn_lon'][i])).meters\n",
    "    start_coordinate_y_tmp = geodesic((start_lat, start_lng), (data['substn_lat'][i], start_lng)).meters\n",
    "\n",
    "    end_coordinate_x_tmp = geodesic((start_lat, start_lng), (start_lat, data['substn_lon'][i])).meters\n",
    "    end_coordinate_y_tmp = geodesic((start_lat, start_lng), (data['substn_lat'][i], start_lng)).meters\n",
    "\n",
    "    start_coordinate_x.append(start_coordinate_x_tmp)\n",
    "    start_coordinate_y.append(start_coordinate_y_tmp)\n",
    "\n",
    "    end_coordinate_x.append(end_coordinate_x_tmp)\n",
    "    end_coordinate_y.append(end_coordinate_y_tmp)\n",
    "\n",
    "data['substn_x'] = start_coordinate_x\n",
    "data['substn_y'] = start_coordinate_y\n",
    "data['substn_x'] = end_coordinate_x\n",
    "data['substn_y'] = end_coordinate_y\n",
    "    \n",
    "subway_xy = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_xy = subway_xy[(subway_xy['substn_x']>500)&(subway_xy['substn_x']<5000)&(subway_xy['substn_y']>500)&(subway_xy['substn_y']<4000)]\n",
    "subway_xy_uniq = subway_xy.drop([108,110], axis=0).drop(['substn_lon','substn_lat'], axis=1).reset_index(drop=True).reset_index(drop=True)\n",
    "subway_xy_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stn = len(subway_xy_uniq)\n",
    "A_mat_subway = [[0 for x in range(num_stn)] for y in range(num_stn)]\n",
    "\n",
    "for i in range(num_stn):\n",
    "    for j in range(num_stn):\n",
    "        A_mat_subway[i][j] = (((subway_xy_uniq.iloc[i][2]-subway_xy_uniq.iloc[j][2])**2+(subway_xy_uniq.iloc[i][3]-subway_xy_uniq.iloc[j][3])**2)**0.5)/1000.\n",
    "A_mat_subway = np.asarray(A_mat_subway)\n",
    "A_mat_subway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation for bus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw/subway_and_bus_station')\n",
    "bus_station = pd.read_csv('bus_station_deer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bus_station\n",
    "data = data[(data['busstn_lat']>start_lat) & (data['busstn_lon']>start_lng) & (data['busstn_lat']>start_lat) & (data['busstn_lon']>start_lng)]\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "    \n",
    "start_coordinate_x = []\n",
    "start_coordinate_y = []\n",
    "end_coordinate_x = []\n",
    "end_coordinate_y = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    start_coordinate_x_tmp = geodesic((start_lat, start_lng), (start_lat, data['busstn_lon'][i])).meters\n",
    "    start_coordinate_y_tmp = geodesic((start_lat, start_lng), (data['busstn_lat'][i], start_lng)).meters\n",
    "\n",
    "    end_coordinate_x_tmp = geodesic((start_lat, start_lng), (start_lat, data['busstn_lon'][i])).meters\n",
    "    end_coordinate_y_tmp = geodesic((start_lat, start_lng), (data['busstn_lat'][i], start_lng)).meters\n",
    "\n",
    "    start_coordinate_x.append(start_coordinate_x_tmp)\n",
    "    start_coordinate_y.append(start_coordinate_y_tmp)\n",
    "\n",
    "    end_coordinate_x.append(end_coordinate_x_tmp)\n",
    "    end_coordinate_y.append(end_coordinate_y_tmp)\n",
    "\n",
    "data['busstn_x'] = start_coordinate_x\n",
    "data['busstn_y'] = start_coordinate_y\n",
    "data['busstn_x'] = end_coordinate_x\n",
    "data['busstn_y'] = end_coordinate_y\n",
    "    \n",
    "bus_xy = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_xy = bus_xy[(bus_xy['busstn_x']>500)&(bus_xy['busstn_x']<5000)&(bus_xy['busstn_y']>500)&(bus_xy['busstn_y']<4000)]\n",
    "bus_xy_uniq = bus_xy.drop_duplicates(subset='busstn_name').drop(['busstn_lon','busstn_lat'], axis=1).reset_index(drop=True)\n",
    "bus_xy_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stn = len(bus_xy_uniq)\n",
    "A_mat_bus = [[0 for x in range(num_stn)] for y in range(num_stn)]\n",
    "\n",
    "for i in range(num_stn):\n",
    "    for j in range(num_stn):\n",
    "        A_mat_bus[i][j] = (((bus_xy_uniq.iloc[i][2]-bus_xy_uniq.iloc[j][2])**2+(bus_xy_uniq.iloc[i][3]-bus_xy_uniq.iloc[j][3])**2)**0.5)/1000.\n",
    "A_mat_bus = np.asarray(A_mat_bus)\n",
    "A_mat_bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**필요한 함수 정의하기**\n",
    "* get_normalized_adj() : A_wave를 만들기 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_adj(A):\n",
    "    A = A + np.diag(np.ones(A.shape[0], dtype=np.float32))\n",
    "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
    "    D[D <= 10e-5] = 10e-5\n",
    "    diag = np.reciprocal(np.sqrt(D))\n",
    "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A), diag.reshape((1, -1)))\n",
    "    return A_wave\n",
    "A_mat_subway = torch.FloatTensor(get_normalized_adj(A_mat_subway))\n",
    "A_mat_bus = torch.FloatTensor(get_normalized_adj(A_mat_bus))\n",
    "\n",
    "\n",
    "A_mat = {}\n",
    "\n",
    "k=12\n",
    "A_mat[k] = A_mat_subway\n",
    "k=204\n",
    "A_mat[k] = A_mat_bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prepare dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'02_data_preprocessing')\n",
    "od_mat_nearestpt = pd.read_csv('14_od_mat_nearestpt.csv')\n",
    "miss_o_nearestpt = pd.read_csv('15_miss_o_nearestpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearestpt = np.concatenate((od_mat_nearestpt[['use_start_at_round', 'near_subway_idx', 'near_bus_idx']],miss_o_nearestpt[['walk_fore_at_round', 'near_subway_idx', 'near_bus_idx']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split train and test\n",
    "- time demand array 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2019-09-08 00:00:00'\n",
    "# train_start = datetime.strptime('2019-09-08 00:00:00','%m-%d-%y ')\n",
    "train_end = '2019-09-28 19:00:00'\n",
    "test_start = '2019-09-28 20:00:00'\n",
    "test_end = '2019-10-11 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=12\n",
    "date_time_range = pd.date_range(start=train_start, end=train_end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "train_demand_subway = pd.DataFrame(0, index = date_time_range, columns=range(k))\n",
    "\n",
    "for i in range(len(nearestpt)):\n",
    "    for j in range(len(train_demand_subway)):\n",
    "        if nearestpt[i][0] == train_demand_subway.index[j]:\n",
    "            train_demand_subway.iloc[j,nearestpt[i][1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=12\n",
    "date_time_range = pd.date_range(start=test_start, end=test_end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "test_demand_subway = pd.DataFrame(0, index = date_time_range, columns=range(k))\n",
    "\n",
    "for i in range(len(nearestpt)):\n",
    "    for j in range(len(test_demand_subway)):\n",
    "        if nearestpt[i][0] == test_demand_subway.index[j]:\n",
    "            test_demand_subway.iloc[j,nearestpt[i][1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=204\n",
    "date_time_range = pd.date_range(start=train_start, end=train_end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "train_demand_bus = pd.DataFrame(0, index = date_time_range, columns=range(k))\n",
    "\n",
    "for i in range(len(nearestpt)):\n",
    "    for j in range(len(train_demand_bus)):\n",
    "        if nearestpt[i][0] == train_demand_bus.index[j]:\n",
    "            train_demand_bus.iloc[j,nearestpt[i][2]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=204\n",
    "date_time_range = pd.date_range(start=test_start, end=test_end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "test_demand_bus = pd.DataFrame(0, index = date_time_range, columns=range(k))\n",
    "\n",
    "for i in range(len(nearestpt)):\n",
    "    for j in range(len(test_demand_bus)):\n",
    "        if nearestpt[i][0] == test_demand_bus.index[j]:\n",
    "            test_demand_bus.iloc[j,nearestpt[i][2]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**put into DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [12, 204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_demand = {}\n",
    "test_time_demand = {}\n",
    "\n",
    "k=12\n",
    "train_time_demand[k] = np.asarray(train_demand_subway).transpose()\n",
    "test_time_demand[k] = np.asarray(test_demand_subway).transpose()\n",
    "k=204\n",
    "train_time_demand[k] = np.asarray(train_demand_bus).transpose()\n",
    "test_time_demand[k] = np.asarray(test_demand_bus).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "\n",
    "holiday = pd.read_csv('holiday_deer.csv')\n",
    "weather = pd.read_csv('weather_deer.csv')\n",
    "weather = weather.iloc[:793]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create node_feature matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_dataset(demand, holiday, weather, k):\n",
    "    time_len = len(demand[k][0])\n",
    "    time_slice = np.zeros((time_len-5, k, (5+2)*5-2))\n",
    "    for idx in range(time_len-5):\n",
    "        demand_time_slice = np.zeros((k, 5+2))\n",
    "        for c in range(k):\n",
    "            demand_k_min = demand[k][c,idx:idx+5].min()\n",
    "            demand_k_max = demand[k][c,idx:idx+5].max()\n",
    "\n",
    "            if (demand_k_max-demand_k_min)==0:\n",
    "                demand_k_normalize = demand[k][c,idx:idx+5]\n",
    "            else:\n",
    "                demand_k_normalize = (demand[k][c,idx:idx+5]-demand_k_min)/(demand_k_max-demand_k_min)\n",
    "\n",
    "            demand_time_slice[c] = np.append(demand_k_normalize, [demand_k_min, demand_k_max])\n",
    "        \n",
    "        holiday_normalize = holiday['holidays'].iloc[idx:idx+5]\n",
    "        holiday_time_slice = np.tile(holiday_normalize, (k, 1))\n",
    "        \n",
    "        temp_min = weather['temperature'].iloc[idx:idx+5].min()\n",
    "        temp_max = weather['temperature'].iloc[idx:idx+5].max()\n",
    "        if (temp_max-temp_min)==0 and temp_min==0:\n",
    "            temp_normalize = weather['temperature'].iloc[idx:idx+5]\n",
    "        elif (temp_max-temp_min)==0 and temp_min!=0:\n",
    "            temp_normalize = weather['temperature'].iloc[idx:idx+5]/temp_min\n",
    "        else:\n",
    "            temp_normalize = (weather['temperature'].iloc[idx:idx+5]-temp_min)/(temp_max-temp_min)\n",
    "#         temp_normalize = (weather['temperature'].iloc[idx:idx+5]-temp_min)/(temp_max-temp_min)\n",
    "        temp_time_slice = np.tile(np.append(temp_normalize, [temp_min, temp_max]), (k, 1))\n",
    "                          \n",
    "        rain_min = weather['rainfall'].iloc[idx:idx+5].min()\n",
    "        rain_max = weather['rainfall'].iloc[idx:idx+5].max()\n",
    "        if (rain_max-rain_min)==0 and rain_min==0:\n",
    "            rain_normalize = weather['rainfall'].iloc[idx:idx+5]\n",
    "        elif (rain_max-rain_min)==0 and rain_min!=0:\n",
    "            rain_normalize = weather['rainfall'].iloc[idx:idx+5]/rain_min\n",
    "        else:\n",
    "            rain_normalize = (weather['rainfall'].iloc[idx:idx+5]-rain_min)/(rain_max-rain_min)\n",
    "        rain_time_slice = np.tile(np.append(rain_normalize, [rain_min, rain_max]), (k, 1))                   \n",
    "\n",
    "        wind_min = weather['windspeed'].iloc[idx:idx+5].min()\n",
    "        wind_max = weather['windspeed'].iloc[idx:idx+5].max()\n",
    "        if (wind_max-wind_min)==0 and wind_min==0:\n",
    "            wind_normalize = weather['windspeed'].iloc[idx:idx+5]\n",
    "        elif (wind_max-wind_min)==0 and wind_min!=0:\n",
    "            wind_normalize = weather['windspeed'].iloc[idx:idx+5]/wind_min\n",
    "        else:\n",
    "            wind_normalize = (weather['windspeed'].iloc[idx:idx+5]-wind_min)/(wind_max-wind_min)\n",
    "        wind_time_slice = np.tile(np.append(wind_normalize, [wind_min, wind_max]), (k, 1))         \n",
    "\n",
    "        time_slice[idx] = np.concatenate((demand_time_slice, holiday_time_slice, temp_time_slice, rain_time_slice, wind_time_slice), axis=1)\n",
    "    return time_slice\n",
    "\n",
    "def create_y_dataset(data, k):\n",
    "    time_len = len(data[k][0])\n",
    "    time_slice = np.zeros((time_len-5, k))\n",
    "    for idx in range(time_len-5):\n",
    "        time_slice[idx] = data[k][:,idx+5]\n",
    "    return time_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_x = {}\n",
    "train_all_y = {}\n",
    "\n",
    "test_all_x = {}\n",
    "test_all_y = {}\n",
    "\n",
    "for k in k_list:\n",
    "    train_all_x[k] = create_x_dataset(train_time_demand, holiday.iloc[0:500], weather.iloc[0:500], k)\n",
    "    train_all_y[k] = create_y_dataset(train_time_demand, k)\n",
    "\n",
    "    test_all_x[k] = create_x_dataset(test_time_demand, holiday.iloc[500:], weather.iloc[500:], k)\n",
    "    test_all_y[k] = create_y_dataset(test_time_demand, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "with open('test_all_y_pt.p', 'wb') as f:\n",
    "    pickle.dump(test_all_y, f)\n",
    "with open('test_time_demand_pt.p', 'wb') as f:\n",
    "    pickle.dump(test_time_demand, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_all = {}\n",
    "train_loader_all = {}\n",
    "test_ds_all = {}\n",
    "test_loader_all = {}\n",
    "\n",
    "for k in k_list:\n",
    "    train_all_x[k] = torch.FloatTensor(train_all_x[k])\n",
    "    train_all_y[k] = torch.FloatTensor(train_all_y[k])\n",
    "    test_all_x[k] = torch.FloatTensor(test_all_x[k])\n",
    "    test_all_y[k] = torch.FloatTensor(test_all_y[k])\n",
    "    \n",
    "    train_ds_all[k] = TensorDataset(train_all_x[k], train_all_y[k])\n",
    "    train_loader_all[k] = DataLoader(train_ds_all[k], batch_size=64, shuffle=False)\n",
    "    \n",
    "    test_ds_all[k] = TensorDataset(test_all_x[k], test_all_y[k])\n",
    "    test_loader_all[k] = DataLoader(test_ds_all[k], batch_size=288, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "with open('train_ds_all_pt.p', 'wb') as f:\n",
    "    pickle.dump(train_ds_all, f)\n",
    "with open('train_loader_all_pt.p', 'wb') as f:\n",
    "    pickle.dump(train_loader_all, f)\n",
    "with open('test_ds_all_pt.p', 'wb') as f:\n",
    "    pickle.dump(test_ds_all, f)\n",
    "with open('test_loader_all_pt.p', 'wb') as f:\n",
    "    pickle.dump(test_loader_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCN Formulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "with open('train_ds_all_pt.p', 'rb') as f:\n",
    "    train_ds_all = pickle.load(f)\n",
    "with open('train_loader_all_pt.p', 'rb') as f:\n",
    "    train_loader_all = pickle.load(f)\n",
    "with open('test_ds_all_pt.p', 'rb') as f:\n",
    "    test_ds_all = pickle.load(f)\n",
    "with open('test_loader_all_pt.p', 'rb') as f:\n",
    "    test_loader_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in test_loader_all[k]:\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.0, use_activation=True, bias=False,\n",
    "                 use_skip_connection=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_activation = use_activation\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        # PyTorch initialization (see docs)\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, X):\n",
    "        node_features, adj_matrix = X\n",
    "        \n",
    "        x = torch.einsum('ijk,km->ijm', node_features.cpu(), self.weight.cpu()).cuda()\n",
    "        x = torch.einsum('ii,jik->jik', adj_matrix.cpu(), x.cpu()).cuda()\n",
    "\n",
    "        # Add bias if necessary\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "\n",
    "        # Apply activation\n",
    "        if self.use_activation:\n",
    "            x = F.relu(x)\n",
    "\n",
    "        if self.use_skip_connection:\n",
    "            x += node_features\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return [x, adj_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gcn(nn.Module):\n",
    "    def __init__(self, nfeatures, nclasses, hidden_size=16, nhidden_layers=0, dropout=0.0,\n",
    "                 use_skip_connection=False):\n",
    "        super(Gcn, self).__init__()\n",
    "\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nclasses = nclasses\n",
    "        self.nfeatures = nfeatures\n",
    "        self.nhidden_layers = nhidden_layers\n",
    "\n",
    "        self.input_layer = GraphConvolution(nfeatures, hidden_size, self.dropout)\n",
    "\n",
    "        # do not use dropout in hidden layers\n",
    "        self.hidden_layers = self.create_conv_sequence(self.nhidden_layers, self.hidden_size,\n",
    "                                                       use_skip_connection=self.use_skip_connection)\n",
    "        self.output_layer = GraphConvolution(hidden_size, nclasses, self.dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_conv_sequence(num_of_layers, hidden_size, use_skip_connection=False):\n",
    "        seq_conv = nn.Sequential()\n",
    "        for i in range(num_of_layers):\n",
    "            seq_conv.add_module(\"Conv \" + str(i),\n",
    "                                GraphConvolution(hidden_size,\n",
    "                                                 hidden_size,\n",
    "                                                 use_skip_connection=use_skip_connection))\n",
    "        return seq_conv\n",
    "\n",
    "    def forward(self, features, adj_matrix):\n",
    "        x = self.input_layer([features, adj_matrix])\n",
    "        x = self.hidden_layers(x)\n",
    "        output = self.output_layer(x)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train process가 이루어지도록 하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, A_mat, loader_all, optimizer, epochs, k, n_layers, dropout, lr, step_size, gamma):\n",
    "\n",
    "    model.train()\n",
    "    loss = nn.MSELoss()\n",
    "    \n",
    "    train_history = {\"train_loss\": []}\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        for train_x_data, train_y_data in loader_all:\n",
    "            train_x_data.cuda()\n",
    "            train_y_data.cuda()\n",
    "            output = model(train_x_data, A_mat)\n",
    "            output = output.squeeze()\n",
    "            loss_train = loss(output.cuda(), train_y_data.cuda()) \n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "        # save current loss\n",
    "        scheduler.step()\n",
    "        train_history[\"train_loss\"].append(loss_train.item())\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(\"epochs: {}, loss: {}\".format(i, loss_train))\n",
    "        \n",
    "        # used to check lr of optimizer\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             print('{}th epoch, the state of scheduler is {}'.format(i, param_group['lr']))\n",
    "\n",
    "#     plt.plot(train_history['train_loss'], label='Train History')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(\"{}_k_{}_hl_{}_do_{}_lr_{}_ss_{}_g_{}_loss.png\".format(k, n_layers, dropout, lr, step_size, gamma, train_history['train_loss'][-1]))\n",
    "#     plt.show()\n",
    "        \n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test process가 이루어지도록 하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, A_mat, loader_all, k):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        test_history = {\"test_loss\": []}\n",
    "\n",
    "        for test_x_data, test_y_data in loader_all:\n",
    "            test_x_data.cuda()\n",
    "            test_y_data.cuda()\n",
    "            output = model(test_x_data, A_mat)\n",
    "            output = output.squeeze()\n",
    "            loss_test = loss(output.cuda(), test_y_data.cuda())\n",
    "        # save current loss\n",
    "        test_history[\"test_loss\"].append(loss_test.item())\n",
    "\n",
    "    return test_history, output, test_y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "hidden_size = 32\n",
    "dropout = 0.0\n",
    "\n",
    "n_layers = 4\n",
    "n_iteration = 10\n",
    "\n",
    "# Train parameters (Adam optimizer is used)\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "step_size = 150\n",
    "gamma = 0.2\n",
    "\n",
    "# setting filename\n",
    "plt_date = \"0728 01 gcn-pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCN formulating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'/04_result_analysis/')\n",
    "\n",
    "# Without skip connection\n",
    "\n",
    "wo_skip_train_history = []\n",
    "wo_skip_test_results = []\n",
    "\n",
    "for k in k_list:\n",
    "    for iteration in range(n_iteration):\n",
    "        print(\"==============================\")\n",
    "        print(\"number of cluster:\", k)\n",
    "        print(\"number of iteration:\", iteration)\n",
    "        model = Gcn(nfeatures=33,\n",
    "                    nhidden_layers=n_layers, \n",
    "                    hidden_size= hidden_size,\n",
    "                    nclasses= 1,\n",
    "                    dropout=dropout,\n",
    "                    use_skip_connection=False)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            A_mat[k] = A_mat[k].cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        plt_name = \" k=\"+str(k)+\" \"\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "        train_losses = train(model, A_mat[k], train_loader_all[k], optimizer, epochs, k, n_layers, dropout, lr, step_size, gamma)\n",
    "        \n",
    "        matplotlib.rcdefaults()\n",
    "        plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "        plt.rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('training and validation loss'+plt_name)\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim(0, 40)\n",
    "        plt.plot(train_losses['train_loss'], label='Train History')\n",
    "        plt.legend()\n",
    "        plt.savefig(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\".png\", quality=100, optimize=True, progressive=True)\n",
    "        \n",
    "        test_losses, test_pred, test_true = test(model, A_mat[k], test_loader_all[k], k)\n",
    "        \n",
    "        np.savetxt(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\"_pred.csv\", test_pred.detach().cpu().numpy().reshape(test_pred.shape[0:2]), delimiter=\",\")\n",
    "        np.savetxt(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\"_true.csv\", test_true.detach().cpu().numpy().reshape(test_true.shape[0:2]), delimiter=\",\")\n",
    "\n",
    "        print(\"Training, Test loss: ({:.6f}, {:.6f})\".format(train_losses['train_loss'][-1], test_losses['test_loss'][-1]))\n",
    "\n",
    "        wo_skip_train_history.append(train_losses)\n",
    "        wo_skip_test_results.append(test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results and Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Structure**\n",
    "\n",
    "* Grid size\n",
    " * 14*18 (252개)\n",
    "\n",
    "* LSTM\n",
    " * Grid structure를 활용하여 비교\n",
    "\n",
    "* GCN\n",
    " * Grid는 모델이 수렴하지 않아서 포함하지 못함.\n",
    "\n",
    "| Number of nodes | Historical Average | LSTM with Grid | GCN with Clluster |\n",
    "|---|:---:|---:|\n",
    "|252| 0.533 | 1.271 | 0.790 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Structure**\n",
    "\n",
    "* Cluster\n",
    " * K가 특정 지점에서만 HA보다 좋음.\n",
    "\n",
    "![cluster](01 figures/figure24.png)\n",
    "\n",
    "* Subway\n",
    " * K=12\n",
    "\n",
    "* Bus\n",
    " * K=204\n",
    "\n",
    "| | Number of nodes | Historical Average | GCN |\n",
    "|---|---|:---:|---:|\n",
    "|Subway| 12 | 39.356 | 54.933 |\n",
    "|Bus | 204 | 0.834 | 1.828 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Proposed novel graph definition for free-floating shared personal mobilities.\n",
    " * For implementing novel deep learning approach (i.e. GCN)\n",
    "\n",
    "* Considered potential unmet demand in the data\n",
    " * If not, the historical demand data would have been underestimated\n",
    " * Unmet demand를 고려한 수요예측으로 추후 재배치에서 활용될 수 있도록 했다.\n",
    " * 이를 통해 unmet demand를 줄일 수 있을 것으로 기대됨.\n",
    "\n",
    "* 같은 cluster 수에서 LSTM보다 GCN이 더 잘맞는다.\n",
    "\n",
    "* GCN은 특정 cluster 수에서만 HA보다 잘맞는다.\n",
    "\n",
    "* Cluster 수가 늘어나면서 데이터가 sparse해질수록 예측하기 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations and Future works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 지역마다 기준이 다르게 적용될 수 있다.\n",
    " * 지리적 특성이나 사람들의 이용 특성에 따라 예측력이 다르게 나타난다.\n",
    "\n",
    "* 피크에 대한 분석이 부족했다.\n",
    " * PeakRMSE and PeakMAE\n",
    "\n",
    "* 적절한 Adjacency matrix에 대한 내용\n",
    " * 추후 연구 필요\n",
    "\n",
    "* Feature extraction이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lin, L., He, Z., & Peeta, S. (2018). Predicting station-level hourly demand in a large-scale bike-sharing network: A graph convolutional neural network approach. Transportation Research Part C: Emerging Technologies, 97(November), 258–276. https://doi.org/10.1016/j.trc.2018.10.011\n",
    "2. Kim, T. S., Lee, W. K., & Sohn, S. Y. (2019). Graph convolutional network approach applied to predict hourly bike-sharing demands considering spatial, temporal, and global effects. PLoS ONE, 14(9), 1–16. https://doi.org/10.1371/journal.pone.0220782\n",
    "3. Kipf, T. N., & Welling, M. (2019). Semi-supervised classification with graph convolutional networks. 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, 1–14.\n",
    "4. https://miro.medium.com/max/1070/1*ivtGccpkSXEDVwf9mgyaNg.png\n",
    "5. Yu, B., Yin, H., & Zhu, Z. (2018). Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting. IJCAI International Joint Conference on Artificial Intelligence, 2018-July, 3634–3640. https://doi.org/10.24963/ijcai.2018/505\n",
    "6. Ham, S. W., Cho, J. H., & Kim, D. K. (2020). Spatiotemporal Demand Prediction Model for Personal Mobility with Latent Feature and Deep Learning: Considering its Intrinsic Features. In Preparation.\n",
    "7. Cho, J. H., Ham, S. W., & Kim, D. K. (2020). Enhancing the Accuracy of Peak Hourly Demand in Bike-Sharing Systems using a Graph Convolutional Network with Public Transit Usage Data. Under Review.\n",
    "8. Yang, Y., A. Heppenstall, A. Turner, and A. Comber. (2019). A Spatiotemporal and Graph-Based Analysis of Dockless Bike Sharing Patterns to Understand Urban Flows over the Last Mile. Computers, Environment and Urban Systems, Vol. 77, No. July, p. 101361. https://doi.org/10.1016/j.compenvurbsys.2019.101361."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical and technical support of Deer Corporation is truly appreciated. The assist was provided both in data and the whole process of research.\n",
    "\n",
    "This research was results of a study on the \"HPC Support\" Project, 1 supported by the 'Ministry of Science and ICT' and NIPA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors confirm contribution to the paper as follows: study conception and design: S.-W. Ham, J.-H. Cho; data collection: S.-W. Ham; analysis and interpretation of results: S.-W. Ham, J.-H. Cho; draft manuscript preparation: S.-W. Ham, J.-H. Cho.\n",
    "\n",
    "All authors reviewed the results and approved the final version of the manuscript."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
