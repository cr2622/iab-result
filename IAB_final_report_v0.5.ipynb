{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20-S IAB term project**\n",
    "\n",
    "---\n",
    "# Spatiotemporal Demand Prediction Model for Personal Mobility using a Graph Convolutional Neural Network\n",
    "\n",
    "---\n",
    "## Authors\n",
    "\n",
    "**Seung-Woo Ham**\n",
    "\n",
    "- Github: [Github Profile](https://github.com/seungwooham)\n",
    "- email: [seungwoo.ham@snu.ac.kr](mailto:seungwoo.ham@snu.ac.kr)\n",
    "- For any question, feel free to contact me.\n",
    "\n",
    "**Jung-Hoon Cho**\n",
    "\n",
    "- Github: [Github Profile](https://github.com/cr2622)\n",
    "- Linkedin: [LinkedIn Profile](https://www.linkedin.com/in/junghoon-cho/)\n",
    "- email: [cr2622@snu.ac.kr](mailto:cr2622@snu.ac.kr)\n",
    "\n",
    "---\n",
    "## Data acquisition\n",
    "\n",
    "- Personal mobility data\n",
    "    * Deer Corporation([webpage](https://deering.co//))\n",
    "\n",
    "- Public transit usage data\n",
    "    * acquired in Seoul Metropolitan Government([http://data.seoul.go.kr/](http://data.seoul.go.kr/))\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- Transportation Laboratory, Seoul National University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* Introduction\n",
    "* Literature Review\n",
    " * Personal mobility\n",
    " * Demand prediction\n",
    "* Methodology\n",
    " * Graph Convolutional Network (GCN)\n",
    " * K-means clustering\n",
    " * others...\n",
    "* Empirical Analysis\n",
    " * Data description\n",
    " * Data preprocessing\n",
    " * Graph definition\n",
    "* Result and discussions\n",
    "* Conclusions\n",
    "* References\n",
    "* Acknowledgment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backgrounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ìš°ë¦¬ë‚˜ë¼ì—ì„œ ìµœê·¼ ì£¼ëª© ë°›ê³  ìˆëŠ” ì „ë™ í‚¥ë³´ë“œ (Personal Mobility) ê³µìœ  ì„œë¹„ìŠ¤\n",
    " * ìˆ˜ìš” ì˜ˆì¸¡ê³¼ ì¬ë°°ì¹˜ì˜ ì–´ë ¤ì›€\n",
    " * ê³ ê°ì˜ ì£¼ìš” ë¶ˆë§Œ ì‚¬í•­ ì¤‘ í•˜ë‚˜ê°€ í‚¥ë³´ë“œì˜ ë¶€ì¬\n",
    " * ì¬ë°°ì¹˜ ì¸ë ¥ ë°°ì¹˜ì— ë§ì€ ì¸ê±´ë¹„ê°€ ì†Œë¹„ë¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì •í™•í•œ ì‹œê³µê°„ì  ìˆ˜ìš” ì˜ˆì¸¡ì„ í†µí•˜ì—¬ ì„œë¹„ìŠ¤ì˜ í’ˆì§ˆê³¼ ê²½ì œì„± í™•ë³´ ê°€ëŠ¥\n",
    " * ì‹œê³µê°„ì  íŒ¨í„´ì„ ë°˜ì˜í•˜ëŠ” ìˆ˜ìš”ì˜ˆì¸¡\n",
    " * Graph Convolutional Neural Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![background](01 figures/figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì „ë™ í‚¥ë³´ë“œì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì‹œê³µê°„ì  ìˆ˜ìš” ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\n",
    " * ì‹¤ì œ ì´ìš© ê¸°ë¡/ì–´í”Œë¦¬ì¼€ì´ì…˜ ì‚¬ìš© ê¸°ë¡ ë“±ì„ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ ì •í™•ë„ ê°œì„ \n",
    " * **Graph Convolutional Neural Network (GCN)** ì ìš©\n",
    "* ì‹œìŠ¤í…œì„ Graphë¡œ ì •ì˜í•˜ëŠ” í˜•íƒœ í•„ìš”\n",
    " * ì´ìš© íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” graphê°€ ë” ì •í™•í•œ ìˆ˜ìš”ì˜ˆì¸¡ì— ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤.\n",
    " * ì–´ë– í•œ í˜•íƒœê°€ ë” ì ì ˆí•œ ê·¸ë˜í”„ì¸ì§€ í™•ì¸ í•„ìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²´ì ì¸ ì—°êµ¬ì˜ íë¦„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "ìš°ì„  ì „ì²´ì ì¸ ì—°êµ¬ì˜ ë°°ê²½ê³¼ ì—°êµ¬ì˜ ëª©ì ì„ ì„¤ëª…í•˜ê³ , ê´€ë ¨ëœ ì„ í–‰ ì—°êµ¬ë¥¼ ê²€í† í•´ë³´ì•˜ë‹¤. ì„ í–‰ì—°êµ¬ì™€ ì°¨ë³„ì ì´ ì–´ë–¤ ê²ƒì´ ë  ìˆ˜ ìˆì„ì§€ ì•Œì•„ë³´ê³ , ì´ ì—°êµ¬ì—ì„œ ì‚¬ìš©í•  ë°©ë²•ë¡ ì— ëŒ€í•´ ì„¤ëª…í•œë‹¤. ì´ë²ˆ ì—°êµ¬ì—ì„œëŠ” Graph Convolutional Network (GCN)ê³¼ K-means clustering ë“± ë‹¤ì–‘í•œ ë°©ë²•ë¡ ì´ ì‚¬ìš©ëœë‹¤. ì´í›„ ì´ì–´ì§€ëŠ” Empirical Analysisì—ì„œëŠ” ì‚¬ìš©í•  ë°ì´í„°ì— ëŒ€í•œ ì†Œê°œì™€ ë°ì´í„° ì „ì²˜ë¦¬, GCN ì˜ˆì¸¡ ëª¨ë¸ì„ í™œìš©í•œ ìˆ˜ìš” ì˜ˆì¸¡ê³¼ ê·¸ ì˜ˆì¸¡ë ¥ í‰ê°€ì™€ ë¶„ì„ì´ ì´ë£¨ì–´ì§„ë‹¤. ë§ˆì§€ë§‰ ê²°ë¡ ì—ì„œëŠ” ì´ë²ˆ ì—°êµ¬ì˜ ê²°ê³¼ì— ëŒ€í•œ í•´ì„ê³¼ ì œì–¸ ë“±ì´ ì´ë£¨ì–´ì§„ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![research_process](01 figures/figure2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lin et al. (2018), Predicting station-level hourly demand in a large-scale bike-sharing network: A graph convolutional neural network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GCN with Data-driven Graph Filter (GCNN-DDGF) \n",
    " * regular GCNN-DDGF: convolution block and feedforward block\n",
    " * recurrent GCNN-DDGF: convolution block, feedforward block and recurrent block from LSTM\n",
    "* GCN with adjacency matrices \n",
    " * Spatial Distance matrix (SD), Demand matrix (DE), Average Trip Duration matrix (ATD), and Demand Correlation matrix (DC). \n",
    " * Evaluation Criteria : MAE, R^2\n",
    " * GCNN_recâˆ’DDGF and GCNN_regâˆ’DDGF perform the best on all criteria\n",
    "* Graph Network Analysis to understand \"black boxâ€œ of GCNN-DDGF\n",
    " * Weighted Degree (WD) : weighted sum of its edges\n",
    " * DDGF not only captures some of the same information existing in the SD, DE and DC matrices, but also uncovers hidden correlations among stations that are not revealed by any of these matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lin2018](01 figures/figure3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kim et al. (2019), Graph convolutional network approach applied to predict hourly bike-sharing demands considering spatial, temporal, and global effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GCN Framework\n",
    " * Repeated Temporal Pattern: hourly, daily and weekly data. -> concatenate -> FC Layer\n",
    " * Global Variables: weather, weekday/weekend\n",
    "* Adjacency Matrices\n",
    " * GCN-Inverse Distance Weight (GCN-IDW)\n",
    " * GCN-Usage Pattern (GCN-UP) have best performance\n",
    " * Evaluation Criteria : MSE, Average Pearson Correlation(APC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kim2019](01 figures/figure4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yang et al. (2019), A spatiotemporal and graph-based analysis of dockless bike sharing patterns to understand urban flows over the last mile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Graph- based analysis\n",
    " * Geostatistical analyses support understanding of large-scale changes in spatiotemporal travel behaviours and graph-based approaches allow changes in local travel flows between individual locations to be quantified and characterized.\n",
    " * The results show how the new metro service boosted nearby bike demand, but with considerable spatial variation, and changed the spatiotemporal patterns of bike travel behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![yang2019](01 figures/figure5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cho et al. (2020), Enhancing the Accuracy of Peak Hourly Demand in Bike-Sharing Systems using a Graph Convolutional Network with Public Transit Usage Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì—°êµ¬ ëŒ€ìƒ: ì„œìš¸ì‹œ ê³µê³µìì „ê±° ë”°ë¦‰ì´\n",
    "* ì—°êµ¬ ë²”ìœ„: ì˜ë“±í¬êµ¬ ì—¬ì˜ë„ë™ (Figure 2)\n",
    "* ê¸°ì¡´ì˜ demand correlation ë¶„ì„í•´ë³´ë©´ ê° ì •ë¥˜ì¥ ê°„ ëŒ€ì—¬íŠ¹ì„±ì´ ë³´ì¸ë‹¤. (Figure 4)\n",
    "* GCN í™œìš©í•˜ë©´ Accuracy ë†’ì•„ì§. (Figure 3)\n",
    "* ë²„ìŠ¤ ìŠ¹í•˜ì°¨ ë°ì´í„°ë¥¼ ë„£ì—ˆì„ ë•Œ ìœ ì˜ë¯¸í•œ ì°¨ì´ ë³´ì´ë©°, í•´ë‹¹ ì§€ì—­ì˜ íŠ¹ì„± ë°˜ì˜í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì„. (Figure 5). \n",
    "* íŠ¹íˆ peak ì˜ˆì¸¡ëŠ¥ë ¥ ê°œì„ ì´ ìœ ì˜ë¯¸í•¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cho2020](01 figures/figure6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ham et al. (2020), Spatiotemporal Demand Prediction Model for Personal Mobility with Latent Feature and Deep Learning: Considering its Intrinsic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì—°êµ¬ ëŒ€ìƒ: ê³µìœ  ì „ë™ í‚¥ë³´ë“œ DEER\n",
    "* ì—°êµ¬ ë²”ìœ„: ì„±ë™êµ¬, ê´‘ì§„êµ¬ ì¼ë¶€ ì§€ì—­\n",
    "* Encoder-RNN-Decoder (ERD) ë„ì…\n",
    "* Latent featureë¥¼ ì¶”ì¶œí•˜ëŠ”ë° íš¨ê³¼ì ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ham2020](01 figures/figure7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Neural Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* consider relationship among stations that can be represented in a graphical structure\n",
    "* ê·¸ë˜í”„ì˜ í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì— ì í•©\n",
    "* ImageëŠ” CNN ì´ìš©, Time series ë°ì´í„°ëŠ” RNN ì´ìš©\n",
    "* ê³µê°„ ì •ë³´ë¥¼ nodeì™€ edgeë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ë©´ GCN ì í•©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple architecture of CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simplecnn](01 figures/figure8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple two-layer GCN\n",
    " * First introduced in Kipf et al. (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simplegcn](01 figures/figure9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1~4ì˜ nodeì™€ ì´ë¥¼ ì‡ëŠ” edgeë¡œ ì´ë£¨ì–´ì§„ graph\n",
    "* ê° nodeëŠ” featureë¥¼ ê°–ê³  ìˆìŒ (1ì˜ featureëŠ” [1 0 0])\n",
    "* ì—°ê²°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” Aì™€ ê° nodeì˜ featureë¥¼ ë‚˜íƒ€ë‚´ëŠ” Xë¥¼ êµ¬ì„± ê°€ëŠ¥!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](01 figures/figure10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ê° featureì˜ ì–´ëŠ ì •ë„ì˜  ê°€ì¤‘ì¹˜ë¥¼ ì¤„ ì§€ ê²°ì •í•˜ëŠ” weight matrixë¥¼ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![adjandweightmatrix](01 figures/figure11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [4Ã—4] Ã—[4Ã—3] Ã—[3Ã—8] = [4Ã—8] =  X'\n",
    "* ê¸¸ì´ 8ì˜ latent feature vectorê°€ ë‚˜íƒ€ë‚¨\n",
    "* Adjacency matrixì— ì˜í•´ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆëŠ” nodeë§Œ ì˜í–¥ì„ ë¯¸ì¹¨\n",
    "* ê²°ê³¼ë¡œ ë‚˜íƒ€ë‚œ X'ì€ non-linear activationì— ë„£ìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Architecture of GCN hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gcnhiddenlayer](01 figures/figure12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* STGCN Framework (Yu et al. (2018), Cho et al. (2020))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stgcn](01 figures/figure13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GCN with hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prediction Methodologies\n",
    " * Historical Average\n",
    " * the historical Average (HA) calculates the predicting demand of the next hour on the average of historical demands for the previous five hours at each station\n",
    "\n",
    "\n",
    "* Long Short-Term Memory (LSTM)\n",
    " * a special kind of RNN, capable of learning long-term dependencies\n",
    " * explicitly designed to avoid the long-term dependency problem\n",
    "![lstm](01 figures/figure14.png)\n",
    "\n",
    "* GCN\n",
    " * With different graph structures\n",
    " * GRID/Cluster/Subway/Bus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to determine node?\n",
    " * Grid structure\n",
    " * 14X18 grid (*Walkable distance)\n",
    "* Geographical Cluster\n",
    " * K-means clustering\n",
    " * K = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 30, 40, 50, 75, 100, 150, 200, 250, 252]\n",
    "* Subway station cluster\n",
    " * 12 stations\n",
    "* Bus station cluster\n",
    " * 204 stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph construction](01 figures/figure15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluation Criteria\n",
    " * RMSE and MAE\n",
    "\t![rmse,mae](01 figures/figure16.png)\n",
    "\tWhere, ğ‘€ is the number of hourly time steps, ğ‘ is the number of stations, ğ‘¦_ğ‘–ğ‘— and ğ‘¦Â Ì‚_ğ‘–ğ‘— stand for actual and predicted demand at hourly time step ğ‘– and station ğ‘—, respectively.\n",
    "* Hyperparameter\n",
    " * Grid search\n",
    " * Find optimal value\n",
    " * In this study, we use\n",
    "\tOptimizer = Adam, epochs = 200, n_layers = 4, dropout = False, lr = 0.001, step_size = 150, gamma = 0.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Empirical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Personal mobility transaction data (DEER Corporation)\n",
    "* Rent time, rent location, return time, return location, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time scope**\n",
    "* Total dataset: September 8th , 2019 ~ October 10th , 2019\n",
    "    * Train set: 2019-09-08 00:00:00~2019-09-28 19:00:00 (500 hrs)\n",
    "    * Test set: 2019-09-28 20:00:00~2019-10-11 00:00:00 (292 hrs)\n",
    "* For every hour, with 5 hours backward, predicting 1 hour forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geographical scope**\n",
    "* Seongdong-gu and Gwanjin-gu, Seoul\n",
    "* Near Konkuk University Station, \n",
    "* has both a shopping district and residences. \n",
    "* 12 subway stations, and 330 bus stations (aggregated to 204) are located within the service area\n",
    "* one of which is outside of the service area but is located nearby. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![studyarea](01 figures/figure18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descriptive Statistics of the Dataset**\n",
    "\n",
    "*ì „ë™í‚¥ë³´ë“œ ëŒ€ì—¬ íŠ¹ì„±*\n",
    "\n",
    "* Unmet demandì˜ ë¹„ìœ¨ì´ ì•„ì£¼ ë†’ë‹¤.\n",
    " * ë¹Œë¦¬ê³  ì‹¶ì—ˆì§€ë§Œ ë¹Œë¦¬ì§€ ëª»í•˜ëŠ” ë¹„ìœ¨ì´ ë†’ì•˜ìŒ.\n",
    " * ì „ì²´ì ì¸ ìì „ê±° ìˆ˜ì˜ ë¶€ì¡± ë¬¸ì œë„ ìˆì§€ë§Œ ê³µê°„ì  ë¶ˆê· í˜• ë¬¸ì œê°€ ì»¸ë‹¤.\n",
    "* 15ë¶„ ë‚´ì™¸ì˜ ì´ë™ì´ ëŒ€ë‹¤ìˆ˜ì„.\n",
    " * ê³µê³µìì „ê±° í‰ê·  ëŒ€ì—¬ì‹œê°„ 36.3ë¶„\n",
    " * ì„œìš¸ì‹œ ê³µê³µìì „ê±°ì— ë¹„í•´ ëª©ì í†µí–‰ë³´ë‹¤ ìˆ˜ë‹¨í†µí–‰ì˜ ë¹„ìœ¨ì´ ë†’ë‹¤ê³  ì¶”ì •í•  ìˆ˜ ìˆìŒ.\n",
    " * ì´ë¥¼ ì°¸ê³ í•˜ì—¬ grid size ê²°ì •\n",
    "* í‰ê·  ì´ë™ê±°ë¦¬ëŠ” ì•½ 1.231 km\n",
    " * ì„œìš¸ì‹œ ê³µê³µìì „ê±°ì˜ í‰ê·  ì´ë™ ê±°ë¦¬ 4,272 km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![escooter](01 figures/figure17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid í¬ê¸° ì„¤ì •ì„ ìœ„í•œ ì ‘ê·¼ ì‹œê°„ ê³„ì‚°**\n",
    "\n",
    "* í•´ë‹¹ ì „ë™ í‚¥ë³´ë“œì˜ ì˜ì—… ë²”ìœ„ì—ì„œ ì „ë™ í‚¥ë³´ë“œ ì‚¬ìš©ì„ 15ë¶„ ì´ìƒ ê±·ëŠ” ê²ƒì€ ì ì ˆí•œ ì‚¬ìš©ì´ë¼ ë³´ê¸° ì–´ë ¤ì›€\n",
    "* Walkfore ì§€ì ë¶€í„°, ì´ìš© ì§€ì ê¹Œì§€ì˜ ê±°ë¦¬, ë‚¨ì€ ì‹œê°„ìœ¼ë¡œ ì†ë„ í™•ì¸í•˜ì—¬ ë„ë³´ ì´ìš©ë§Œ ê³¨ë¼ëƒ„\n",
    "* 5m/s ì´í•˜ì˜ ì†ë„ë¡œ ì ‘ê·¼ì¤‘ì´ë©´ì„œ, 15ë¶„ë‚´ì˜ walkforeì™€ ê°€ì¥ ìµœê·¼ ì´ìš©ì„ matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grid](01 figures/figure19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì ‘ê·¼ì‹œê°„ ì‚°ì • ì½”ë“œ**\n",
    "* ì–´í”Œë¦¬ì¼€ì´ì…˜ ë°ì´í„°ì™€ ë§¤ì¹­ëœ tableì„ í™œìš©í•˜ì—¬ ì´ìš©ê¹Œì§€ ê±¸ë¦¬ëŠ” í‰ê·  ì‹œê°„ ì‚°ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cal_grid_size:\n",
    "    def __init__(self, filename):\n",
    "        self.raw_data = pd.read_csv(filename, header=0)\n",
    "        \n",
    "        distance_list = []\n",
    "        \n",
    "        for i in range(len(self.raw_data)):\n",
    "            distance_tmp = geodesic((self.raw_data['walk_fore_lat'][i], self.raw_data['walk_fore_lng'][i]),\n",
    "                                    (self.raw_data['use_start_lat'][i], self.raw_data['use_start_lng'][i])).meters\n",
    "            distance_list.append(distance_tmp)\n",
    "        \n",
    "        self.raw_data['distance'] = distance_list\n",
    "        \n",
    "    def get_time_diff(self):\n",
    "        self.raw_data['walk_fore_at'] = pd.to_datetime(self.raw_data['walk_fore_at'])\n",
    "        self.raw_data['use_start_at'] = pd.to_datetime(self.raw_data['use_start_at'])\n",
    "\n",
    "        self.raw_data['eta'] = self.raw_data['use_start_at'] - self.raw_data['walk_fore_at']\n",
    "        self.raw_data['eta'] = self.raw_data['eta'].dt.total_seconds()\n",
    "        self.raw_data['speed'] = self.raw_data['distance']/self.raw_data['eta']\n",
    "\n",
    "        self.speed_limit = self.raw_data.loc[self.raw_data['speed']<5]\n",
    "        self.speed_limit = self.speed_limit.drop_duplicates(['use_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_grid_tb = cal_grid_size('01_fore_use.csv')\n",
    "cal_grid_tb.get_time_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (\n",
    "    ggplot(cal_grid_tb.speed_limit[['eta', 'distance']], aes(x='eta', y='distance')) + \n",
    "    geom_point(size=0.01) + geom_smooth(span=1, color='darkred', size=1.5) + geom_rug(size=0.01) + \n",
    "    xlab('Time to Start Usage (sec)') + ylab('Distance to E-Scooter (m)') + \n",
    "    ggtitle('Distance to E-Scooter by Time to Start Usage') + \n",
    "    theme(\n",
    "        axis_text_x=element_text(size=10), axis_title_x=element_text(size=13),\n",
    "        axis_text_y=element_text(size=10), axis_title_y=element_text(size=13),\n",
    "        title=element_text(size=15)\n",
    "    )\n",
    ")\n",
    "\n",
    "plot\n",
    "\n",
    "# Uncomment the line below to save the figure\n",
    "# ggsave(plot, 'Distance to E-Scooter by Time to Start Usage.png', dpi=700)\n",
    "\n",
    "\n",
    "# Uncomment the line below to save dataframe as csv\n",
    "# cal_grid_tb.speed_limit.to_csv('02_speed_dup_del.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unmet Demand í™•ì¸ ì½”ë“œ**\n",
    "* Map zoom tableì—ëŠ” ì§€ë„ë¥¼ í™•ëŒ€í•˜ëŠ” í–‰ìœ„ë“¤ì´ ë‹´ê²¨ìˆê³  columnì€ ê°ê° 'map zoom', 'map_zoom_id', 'map_zoom_user_id', 'map_zoom_lat', 'map_zoom_lng', 'map_zoom_level', 'map_zoom_at'ì„\n",
    "* Walk fore tableì—ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ ê¸°ë¡ì´ ìˆìœ¼ë©° columnì€ ê°ê° 'walk_fore_id', 'walk_fore_user_id', 'walk_fore_lat', 'walk_fore_lng', 'walk_fore_at'ì„\n",
    "* Tableì„ 'user id'ì™€ 'time'ì— ëŒ€í•´ ì •ë ¬, ì´í›„ map zoom tableê³¼ walk fore tableì„ left joiní•¨\n",
    "* Left joinëœ tableì€ ë‹¤ì‹œ use tableê³¼ ë¹„êµë˜ì–´ unmet demand í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_zoom = pd.read_csv('03_map_zoom_tb.csv')\n",
    "walk_fore = pd.read_csv('04_walk_fore_tb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_zoom = map_zoom.sort_values(by=['map_zoom_user_id', 'map_zoom_at'])\n",
    "map_zoom = map_zoom.reset_index(drop=True)\n",
    "map_zoom['map_zoom_at'] = pd.to_datetime(map_zoom['map_zoom_at'])\n",
    "map_zoom['map_zoom_at_round'] = map_zoom.map_zoom_at.dt.round('5min')\n",
    "\n",
    "walk_fore = walk_fore.sort_values(by=['walk_fore_user_id', 'walk_fore_at'])\n",
    "walk_fore = walk_fore.reset_index(drop=True)\n",
    "walk_fore['walk_fore_at'] = pd.to_datetime(walk_fore['walk_fore_at'])\n",
    "walk_fore['walk_fore_at_round'] = walk_fore.walk_fore_at.dt.round('5min')\n",
    "walk_fore = walk_fore.drop_duplicates(subset=['walk_fore_user_id', 'walk_fore_at_round'], keep='last')\n",
    "walk_fore = walk_fore.reset_index(drop=True)\n",
    "\n",
    "zoom_fore = pd.merge(map_zoom, walk_fore, how='left', \n",
    "                     left_on=['map_zoom_user_id', 'map_zoom_at_round'], \n",
    "                     right_on=['walk_fore_user_id', 'walk_fore_at_round'])\n",
    "\n",
    "zoom_fore = zoom_fore[np.isfinite(zoom_fore['walk_fore_lng'])]\n",
    "zoom_fore = zoom_fore.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_list = []\n",
    "\n",
    "for i in range(len(zoom_fore)):\n",
    "    distance_tmp = geodesic((zoom_fore['map_zoom_lat'][i], zoom_fore['map_zoom_lng'][i]), \n",
    "                            (zoom_fore['walk_fore_lat'][i], zoom_fore['walk_fore_lng'][i])).meters\n",
    "    distance_list.append(distance_tmp)\n",
    "\n",
    "zoom_fore['distance'] = distance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom_fore = zoom_fore.sort_values(by=['walk_fore_user_id', 'walk_fore_at_round', 'distance'])\n",
    "zoom_fore = zoom_fore.drop_duplicates(subset=['walk_fore_user_id', 'walk_fore_at_round'], keep='first')\n",
    "zoom_fore = zoom_fore[zoom_fore['distance']<251]\n",
    "zoom_fore = zoom_fore.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_use = pd.read_csv('01_fore_use.csv')\n",
    "fore_use.drop_duplicates(subset=['use_id'], keep='first', inplace=True)\n",
    "missing_call = zoom_fore[~zoom_fore['walk_fore_id'].isin(list(set(fore_use['walk_fore_id']) & set(zoom_fore['walk_fore_id'])))]\n",
    "\n",
    "# Uncomment the line below to save dataframe as csv\n",
    "# missing_call.to_csv('06_missing_call.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¤‘ë³µ Row ì²˜ë¦¬**\n",
    "* missing callê³¼ od_mat ì‚¬ì´ì˜ ì¤‘ë³µ ì²˜ë¦¬ë¥¼ ìœ„í•œ od_mat ë‚´ë¶€ ìƒˆë¡œìš´ column ìƒì„±\n",
    "* left onlyë¥¼ í™œìš©í•˜ì—¬ ë‹¤ì‹œ í•œ ë²ˆ missing call ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_mat['use_start_at_round_15min'] = od_mat['use_start_at'].dt.round('15min')\n",
    "miss_o['walk_fore_at_round_15min'] = miss_o['walk_fore_at'].dt.round('15min')\n",
    "miss_o = miss_o.drop_duplicates(['walk_fore_user_id', 'walk_fore_at_round_15min'], keep='first').reset_index(drop=True)\n",
    "\n",
    "miss_o_trim = pd.merge(miss_o, od_mat, left_on=['walk_fore_user_id', 'walk_fore_at_round_15min'], right_on=['use_user_id', 'use_start_at_round_15min'], how='outer', indicator=True)\n",
    "miss_o_trim = miss_o_trim[miss_o_trim['_merge'] == 'left_only']\n",
    "miss_o_trim = miss_o_trim[miss_o_trim.columns[:8]]\n",
    "miss_o_trim = miss_o_trim.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê±°ë¦¬ì— ë”°ë¥¸ triming**\n",
    "* ë¶„ì„ ë²”ìœ„ì˜ ì„¤ì •\n",
    "* ë¶„ì„ ë²”ìœ„ ì™¸ì˜ ë°ì´í„°ëŠ” ì‚­ì œ\n",
    "* lat_start = 37.526531\n",
    "* lng_start = 127.037807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_by_range(data1, data2, start_cor, end_cor):\n",
    "    data1 = data1[(data1['use_start_x']>start_cor[0]) & (data1['use_start_y']>start_cor[1]) &\n",
    "                  (data1['use_end_x']>start_cor[0]) & (data1['use_end_y']>start_cor[1]) &\n",
    "                  (data1['use_start_x']<end_cor[0]) & (data1['use_start_y']<end_cor[1]) &\n",
    "                  (data1['use_end_x']<end_cor[0]) & (data1['use_end_y']<end_cor[1])]\n",
    "    \n",
    "    data2 = data2[(data2['walk_fore_x']>start_cor[0]) & (data2['walk_fore_y']>start_cor[1]) &\n",
    "                  (data2['walk_fore_x']<end_cor[0]) & (data2['walk_fore_y']<end_cor[1])]\n",
    "\n",
    "    data1 = data1.reset_index(drop=True)\n",
    "    data2 = data2.reset_index(drop=True)\n",
    "    \n",
    "    return data1, data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [500, 500]\n",
    "end = [5000, 4000]\n",
    "od_mat, miss_o_trim = cut_by_range(od_mat, miss_o_trim, start, end)\n",
    "# However, it is already done in the previous stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**k-means clustering**\n",
    "- based on geographical location(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = list(range(1,21,1))+list(range(30,51,10))+[75, 100, 150, 200, 250, 252]\n",
    "\n",
    "for k in k_list:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(np.vstack((train_od_mat[['use_start_x', 'use_start_y']].values, train_miss_o[['walk_fore_x', 'walk_fore_y']].values)))\n",
    "    train_cluster = kmeans.predict(np.vstack((train_od_mat[['use_start_x', 'use_start_y']].values, train_miss_o[['walk_fore_x', 'walk_fore_y']].values)))\n",
    "    test_od_mat_cluster = kmeans.predict(test_od_mat[['use_start_x', 'use_start_y']])\n",
    "    test_miss_o_cluster = kmeans.predict(test_miss_o[['walk_fore_x', 'walk_fore_y']])\n",
    "    \n",
    "    train_od_mat[str(k)+' clusters'] = pd.Series(train_cluster[:18674])\n",
    "    train_miss_o[str(k)+' clusters'] = pd.Series(train_cluster[18674:])\n",
    "    test_od_mat[str(k)+' clusters'] = pd.Series(test_od_mat_cluster)\n",
    "    test_miss_o[str(k)+' clusters'] = pd.Series(test_miss_o_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = True # Change to false to write\n",
    "\n",
    "if read == False:\n",
    "    with open('train_od_mat_18674.p', 'wb') as f:\n",
    "        pickle.dump(train_od_mat, f)\n",
    "    with open('train_miss_o_16541.p', 'wb') as f:\n",
    "        pickle.dump(train_miss_o, f)        \n",
    "    with open('test_od_mat_11342.p', 'wb') as f:\n",
    "        pickle.dump(test_od_mat, f)\n",
    "    with open('test_miss_o_11367.p', 'wb') as f:\n",
    "        pickle.dump(test_miss_o, f)\n",
    "\n",
    "else:\n",
    "    with open('train_od_mat_18674.p', 'rb') as f:\n",
    "        train_od_mat = pickle.load(f)\n",
    "    with open('train_miss_o_16541.p', 'rb') as f:\n",
    "        train_miss_o = pickle.load(f)\n",
    "    with open('test_od_mat_11342.p', 'rb') as f:\n",
    "        test_od_mat = pickle.load(f)\n",
    "    with open('test_miss_o_11367.p', 'rb') as f:\n",
    "        test_miss_o = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Pre-processing with K-means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_demand_array(data, start, end, k, od_mat):\n",
    "    date_time_range = pd.date_range(start=start, end=end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "    time_demand = np.zeros((k, len(date_time_range)))\n",
    "    \n",
    "    if od_mat==True:\n",
    "        data_k = data[['use_user_id', 'use_start_at', 'use_start_x', 'use_start_y', 'use_end_at', 'use_end_x', 'use_end_y',\n",
    "                       'use_start_at_round', 'use_end_at_round', str(k)+' clusters']]\n",
    "        for idx, date_time in enumerate(date_time_range):\n",
    "            data_k_date_time = data_k[data_k['use_start_at_round']==date_time]\n",
    "            for cluster in range(k):\n",
    "                time_demand[cluster][idx] += (data_k_date_time[str(k)+' clusters']==cluster).sum()\n",
    "    else:\n",
    "        data_k = data[['walk_fore_user_id', 'walk_fore_at', 'walk_fore_x', 'walk_fore_y', 'walk_fore_at_round', str(k)+' clusters']]\n",
    "        for idx, date_time in enumerate(date_time_range):\n",
    "            data_k_date_time = data_k[data_k['walk_fore_at_round']==date_time]\n",
    "            for cluster in range(k):\n",
    "                time_demand[cluster][idx] += (data_k_date_time[str(k)+' clusters']==cluster).sum()\n",
    "                \n",
    "    return time_demand\n",
    "\n",
    "def get_normalized_adj(A):\n",
    "    # A = A + np.diag(np.ones(A.shape[0], dtype=np.float32))\n",
    "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
    "    D[D <= 10e-5] = 10e-5\n",
    "    diag = np.reciprocal(np.sqrt(D))\n",
    "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A), diag.reshape((1, -1)))\n",
    "    return A_wave\n",
    "\n",
    "def create_a(train_od_mat, train_miss_o, k):\n",
    "    avg_x_y = []\n",
    "    train_od_mat_k = train_od_mat[['use_user_id', 'use_start_at', 'use_start_x', 'use_start_y',\n",
    "                                   'use_end_at', 'use_end_x', 'use_end_y', 'use_start_at_round', 'use_end_at_round',\n",
    "                                   str(k)+' clusters']]  \n",
    "    train_miss_o_k = train_miss_o[['walk_fore_user_id', 'walk_fore_at',\n",
    "                                   'walk_fore_x', 'walk_fore_y', 'walk_fore_at_round', str(k)+' clusters']]\n",
    "    for cluster in range(k):\n",
    "        train_od_mat_len = len(train_od_mat[train_od_mat[str(k)+' clusters']==cluster])\n",
    "        train_od_mat_sum_x = train_od_mat[train_od_mat[str(k)+' clusters']==cluster]['use_start_x'].sum()\n",
    "        train_od_mat_sum_y = train_od_mat[train_od_mat[str(k)+' clusters']==cluster]['use_start_y'].sum()\n",
    "\n",
    "        train_miss_o_len = len(train_miss_o[train_miss_o[str(k)+' clusters']==cluster])\n",
    "        train_miss_o_sum_x = train_miss_o[train_miss_o[str(k)+' clusters']==cluster]['walk_fore_x'].sum()\n",
    "        train_miss_o_sum_y = train_miss_o[train_miss_o[str(k)+' clusters']==cluster]['walk_fore_y'].sum()\n",
    "\n",
    "        avg_x = (train_od_mat_sum_x+train_miss_o_sum_x)/(train_od_mat_len+train_miss_o_len)\n",
    "        avg_y = (train_od_mat_sum_y+train_miss_o_sum_y)/(train_od_mat_len+train_miss_o_len)\n",
    "        avg_x_y.append([avg_x, avg_y])\n",
    "    \n",
    "    dist_mat = [[0 for x in range(k)] for y in range(k)] \n",
    "\n",
    "    for i, cluster in enumerate(range(k)):\n",
    "        for j, cluster in enumerate(range(i,k)):\n",
    "            dist_mat[i][k-1-j] = ((avg_x_y[i][0]-avg_x_y[k-1-j][0])**2+(avg_x_y[i][1]-avg_x_y[k-1-j][1])**2)**0.5\n",
    "            dist_mat[k-1-j][i] = dist_mat[i][k-1-j]\n",
    "    \n",
    "    dist_mat = np.asarray(dist_mat)\n",
    "    print('Average distance between stations are {} m'.format(dist_mat.mean()))\n",
    "    \n",
    "    distance_threshold = True\n",
    "    \n",
    "    if distance_threshold == False:\n",
    "        '''\n",
    "        Create matrix with 1 and 0 by distance threshold\n",
    "        '''\n",
    "        dist_mat[np.where(dist_mat <= dist_mat.mean()/5)] = 1\n",
    "        dist_mat[np.where(dist_mat > dist_mat.mean()/5)] = 0\n",
    "\n",
    "        print('From total {}*{}={} combinations, {} pairs are selected it is near each other'.format(k, k, k**2, dist_mat.sum()))\n",
    "        A_mat = torch.FloatTensor(get_normalized_adj(dist_mat))\n",
    "    \n",
    "    else:\n",
    "        '''\n",
    "        Create matrix with reciprocal of distance\n",
    "        '''\n",
    "        for i in range(len(dist_mat)):\n",
    "            dist_mat[i][i] = dist_mat[i].min()\n",
    "        A_mat = torch.FloatTensor(get_normalized_adj(dist_mat))\n",
    "        \n",
    "    return A_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 0 : Grid structure**\n",
    "* Baseline\n",
    "* num_cluster = 14*18=252\n",
    "* Grid Centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![grid](01 figures/figure20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 1 : Geographical Cluster**\n",
    "* K-means clustering\n",
    "* ìƒì„±ëœ Train/Test dataì— cluster ë²ˆí˜¸ ë“±ë¡ ì¤‘\n",
    "* ëŒ€ëµ 15ê°œ ê·¼ë°©ì—ì„œ elbowë¥¼ í™•ì¸í•¨\n",
    "* [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 30, 40, 50, 75, 100, 150, 200, 250, 252]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![geographicalcluster](01 figures/figure21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 2 : Subway station Cluster**\n",
    "* num_cluster = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![subway](01 figures/figure22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 3 : Bus station Cluster**\n",
    "* num_cluster = 204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bus](01 figures/figure23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency matrix and Node feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjacency matrix**\n",
    "* Distance matrix\n",
    "    * ì„¤ì •ëœ ë…¸ë“œ ê°„ì˜ geographical distanceë¥¼ í™œìš©\n",
    "    * A_wave ë§Œë“¤ê¸° ìœ„í•´ get_normalized_adj() í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "\n",
    "**Node feature matrix**\n",
    "* Weather\n",
    " * ê¸°ìƒì²­ ì‹œê°„ëŒ€ë³„ ìë£Œ í™œìš©\n",
    " * temperature, wind, rainfall\n",
    "* Holiday\n",
    " * ê·¸ë‚ ì´ í‰ì¼ì¸ì§€, ì£¼ë§ì¸ì§€ ê³µíœ´ì¼ì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” binary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Average (HA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = list(range(1,21,1))+list(range(30,51,10))+[75, 100, 150, 200, 250, 252]\n",
    "\n",
    "ha = {}\n",
    "\n",
    "for k in k_list:\n",
    "    # print(test_all_demand[k].shape)\n",
    "    time_len = len(test_all_demand[k][0])\n",
    "    time_slice = np.zeros((time_len-5, k))\n",
    "\n",
    "    for idx in range(time_len-5):\n",
    "        ha_time_slice = np.zeros((k))\n",
    "        for c in range(k):\n",
    "            ha_k = test_all_demand[k][c,idx:idx+5].mean()\n",
    "            ha_time_slice[c] = ha_k\n",
    "#         print(ha_time_slice)\n",
    "        time_slice[idx] = ha_time_slice\n",
    "    ha[k] = time_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_HA = {}\n",
    "for k in k_list:\n",
    "    MSE_HA[k] = mean_squared_error(ha[k],test_all_y_dic[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_HA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN - k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2019-09-08 00:00:00'\n",
    "train_end = '2019-09-28 19:00:00'\n",
    "test_start = '2019-09-28 20:00:00'\n",
    "test_end = '2019-10-11 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "\n",
    "# Data with clusters\n",
    "with open('train_od_mat_18674.p', 'rb') as f:\n",
    "    train_od_mat = pickle.load(f)\n",
    "with open('train_miss_o_16541.p', 'rb') as f:\n",
    "    train_miss_o = pickle.load(f)\n",
    "with open('test_od_mat_11342.p', 'rb') as f:\n",
    "    test_od_mat = pickle.load(f)\n",
    "with open('test_miss_o_11367.p', 'rb') as f:\n",
    "    test_miss_o = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "with open('train_ds_all.p', 'rb') as f:\n",
    "    train_ds_all = pickle.load(f)\n",
    "with open('train_loader_all.p', 'rb') as f:\n",
    "    train_loader_all = pickle.load(f)\n",
    "with open('test_ds_all.p', 'rb') as f:\n",
    "    test_ds_all = pickle.load(f)\n",
    "with open('test_loader_all.p', 'rb') as f:\n",
    "    test_loader_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**í•„ìš”í•œ í•¨ìˆ˜ ì •ì˜í•˜ê¸°**\n",
    "* get_normalized_adj() : A_waveë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í•¨ìˆ˜\n",
    "* create_a() : ë‹¤ì–‘í•œ input data(distance)ë¥¼ adjacency matrixì˜ í˜•íƒœë¡œ ë§Œë“¤ê¸° ìœ„í•œ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_adj(A):\n",
    "    A = A + np.diag(np.ones(A.shape[0], dtype=np.float32))\n",
    "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
    "    D[D <= 10e-5] = 10e-5\n",
    "    diag = np.reciprocal(np.sqrt(D))\n",
    "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A), diag.reshape((1, -1)))\n",
    "    return A_wave\n",
    "\n",
    "def create_a(train_od_mat, train_miss_o, k):\n",
    "    avg_x_y = []\n",
    "    train_od_mat_k = train_od_mat[['use_user_id', 'use_start_at', 'use_start_x', 'use_start_y',\n",
    "                                   'use_end_at', 'use_end_x', 'use_end_y', 'use_start_at_round', 'use_end_at_round',\n",
    "                                   str(k)+' clusters']]  \n",
    "    train_miss_o_k = train_miss_o[['walk_fore_user_id', 'walk_fore_at',\n",
    "                                   'walk_fore_x', 'walk_fore_y', 'walk_fore_at_round', str(k)+' clusters']]\n",
    "    for cluster in range(k):\n",
    "        train_od_mat_len = len(train_od_mat[train_od_mat[str(k)+' clusters']==cluster])\n",
    "        train_od_mat_sum_x = train_od_mat[train_od_mat[str(k)+' clusters']==cluster]['use_start_x'].sum()\n",
    "        train_od_mat_sum_y = train_od_mat[train_od_mat[str(k)+' clusters']==cluster]['use_start_y'].sum()\n",
    "\n",
    "        train_miss_o_len = len(train_miss_o[train_miss_o[str(k)+' clusters']==cluster])\n",
    "        train_miss_o_sum_x = train_miss_o[train_miss_o[str(k)+' clusters']==cluster]['walk_fore_x'].sum()\n",
    "        train_miss_o_sum_y = train_miss_o[train_miss_o[str(k)+' clusters']==cluster]['walk_fore_y'].sum()\n",
    "\n",
    "        avg_x = (train_od_mat_sum_x+train_miss_o_sum_x)/(train_od_mat_len+train_miss_o_len)\n",
    "        avg_y = (train_od_mat_sum_y+train_miss_o_sum_y)/(train_od_mat_len+train_miss_o_len)\n",
    "        avg_x_y.append([avg_x, avg_y])\n",
    "    \n",
    "    dist_mat = [[0 for x in range(k)] for y in range(k)] \n",
    "\n",
    "    for i, cluster in enumerate(range(k)):\n",
    "        for j, cluster in enumerate(range(i,k)):\n",
    "            dist_mat[i][k-1-j] = (((avg_x_y[i][0]-avg_x_y[k-1-j][0])**2+(avg_x_y[i][1]-avg_x_y[k-1-j][1])**2)**0.5)/1000.\n",
    "            dist_mat[k-1-j][i] = dist_mat[i][k-1-j]\n",
    "    \n",
    "    dist_mat = np.asarray(dist_mat)\n",
    "    print('For {} clusters, average distance between stations are {} km'.format(k, dist_mat.mean()))\n",
    "    \n",
    "    distance_threshold = False\n",
    "    \n",
    "    if distance_threshold == True:\n",
    "        '''\n",
    "        Create matrix with 1 and 0 by distance threshold\n",
    "        '''\n",
    "        print(dist_mat)\n",
    "        dist_mat[np.where(dist_mat <= dist_mat.mean()/1.5)] = 1\n",
    "        dist_mat[np.where(dist_mat > dist_mat.mean()/1.5)] = 0\n",
    "        \n",
    "        print('From total {}*{}={} combinations, {} pairs are selected it is near each other'.format(k, k, k**2, dist_mat.sum()))\n",
    "        print(dist_mat)\n",
    "        A_mat = torch.FloatTensor(get_normalized_adj(dist_mat))\n",
    "    \n",
    "    else:\n",
    "        '''\n",
    "        Create matrix with reciprocal of distance\n",
    "        '''\n",
    "        A_mat = torch.FloatTensor(get_normalized_adj(dist_mat))\n",
    "        \n",
    "    return A_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê° clusterì— ëŒ€í•´ì„œ A_matë¥¼ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mat = {}\n",
    "\n",
    "k_list = list(range(1,21,1))+list(range(30,51,10))+[75, 100, 150, 200, 250, 252]\n",
    "\n",
    "for k in k_list:\n",
    "    A_mat[k] = create_a(train_od_mat, train_miss_o, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN class ì •ì˜í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.0, use_activation=True, bias=False,\n",
    "                 use_skip_connection=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_activation = use_activation\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        # PyTorch initialization (see docs)\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, X):\n",
    "        node_features, adj_matrix = X\n",
    "        \n",
    "        x = torch.einsum('ijk,km->ijm', node_features.cpu(), self.weight.cpu()).cuda()\n",
    "        x = torch.einsum('ii,jik->jik', adj_matrix.cpu(), x.cpu()).cuda()\n",
    "\n",
    "        # Add bias if necessary\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "\n",
    "        # Apply activation\n",
    "        if self.use_activation:\n",
    "            x = F.relu(x)\n",
    "\n",
    "        if self.use_skip_connection:\n",
    "            x += node_features\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return [x, adj_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gcn(nn.Module):\n",
    "    def __init__(self, nfeatures, nclasses, hidden_size=16, nhidden_layers=0, dropout=0.0,\n",
    "                 use_skip_connection=False):\n",
    "        super(Gcn, self).__init__()\n",
    "\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nclasses = nclasses\n",
    "        self.nfeatures = nfeatures\n",
    "        self.nhidden_layers = nhidden_layers\n",
    "\n",
    "        self.input_layer = GraphConvolution(nfeatures, hidden_size, self.dropout)\n",
    "\n",
    "        # do not use dropout in hidden layers\n",
    "        self.hidden_layers = self.create_conv_sequence(self.nhidden_layers, self.hidden_size,\n",
    "                                                       use_skip_connection=self.use_skip_connection)\n",
    "        self.output_layer = GraphConvolution(hidden_size, nclasses, self.dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_conv_sequence(num_of_layers, hidden_size, use_skip_connection=False):\n",
    "        seq_conv = nn.Sequential()\n",
    "        for i in range(num_of_layers):\n",
    "            seq_conv.add_module(\"Conv \" + str(i),\n",
    "                                GraphConvolution(hidden_size,\n",
    "                                                 hidden_size,\n",
    "                                                 use_skip_connection=use_skip_connection))\n",
    "        return seq_conv\n",
    "\n",
    "    def forward(self, features, adj_matrix):\n",
    "        x = self.input_layer([features, adj_matrix])\n",
    "        x = self.hidden_layers(x)\n",
    "        output = self.output_layer(x)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train processê°€ ì´ë£¨ì–´ì§€ë„ë¡ í•˜ëŠ” í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, A_mat, loader_all, optimizer, epochs, k, n_layers, dropout, lr, step_size, gamma):\n",
    "\n",
    "    model.train()\n",
    "    loss = nn.MSELoss()\n",
    "    \n",
    "    train_history = {\"train_loss\": []}\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        for train_x_data, train_y_data in loader_all:\n",
    "            train_x_data.cuda()\n",
    "            train_y_data.cuda()\n",
    "            output = model(train_x_data, A_mat)\n",
    "            output = output.squeeze()\n",
    "            loss_train = loss(output.cuda(), train_y_data.cuda()) \n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "        # save current loss\n",
    "        scheduler.step()\n",
    "        train_history[\"train_loss\"].append(loss_train.item())\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(\"epochs: {}, loss: {}\".format(i, loss_train))\n",
    "        \n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test processê°€ ì´ë£¨ì–´ì§€ë„ë¡ í•˜ëŠ” í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, A_mat, loader_all, k):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        test_history = {\"test_loss\": []}\n",
    "\n",
    "        for test_x_data, test_y_data in loader_all:\n",
    "            test_x_data.cuda()\n",
    "            test_y_data.cuda()\n",
    "            output = model(test_x_data, A_mat)\n",
    "            output = output.squeeze()\n",
    "            loss_test = loss(output.cuda(), test_y_data.cuda())\n",
    "        # save current loss\n",
    "        test_history[\"test_loss\"].append(loss_test.item())\n",
    "\n",
    "    return test_history, output, test_y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter ì„¤ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = list(range(1,21,1))+list(range(30,51,10))+[75, 100, 150, 200, 250, 252]\n",
    "# k_list = [2, 4, 6, 8, 10, 15, 20, 30, 50, 75, 100, 150, 200, 252]\n",
    "# k_list = [20]\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 32\n",
    "dropout = 0.0\n",
    "\n",
    "n_layers = 4\n",
    "n_iteration = 10\n",
    "\n",
    "# Train parameters (Adam optimizer is used)\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "step_size = 150\n",
    "gamma = 0.2\n",
    "\n",
    "# setting filename\n",
    "plt_date = \"0729 00 gcn-cluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCN formulating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'/04_result_analysis/')\n",
    "\n",
    "# Without skip connection\n",
    "\n",
    "wo_skip_train_history = []\n",
    "wo_skip_test_results = []\n",
    "\n",
    "for k in k_list:\n",
    "    for iteration in range(n_iteration):\n",
    "        print(\"==============================\")\n",
    "        print(\"number of cluster:\", k)\n",
    "        print(\"number of iteration:\", iteration)\n",
    "        model = Gcn(nfeatures=33, \n",
    "                    nhidden_layers=n_layers, \n",
    "                    hidden_size= hidden_size,\n",
    "                    nclasses= 1,\n",
    "                    dropout=dropout,\n",
    "                    use_skip_connection=False)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            A_mat[k] = A_mat[k].cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        plt_name = \" k=\"+str(k)+\" \"\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "        train_losses = train(model, A_mat[k], train_loader_all[k], optimizer, epochs, k, n_layers, dropout, lr, step_size, gamma)\n",
    "        \n",
    "        matplotlib.rcdefaults()\n",
    "        plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "        plt.rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('training and validation loss'+plt_name)\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim(0, 40)\n",
    "        plt.plot(train_losses['train_loss'], label='Train History')\n",
    "        plt.legend()\n",
    "        plt.savefig(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\".png\", quality=100, optimize=True, progressive=True)\n",
    "        \n",
    "        test_losses, test_pred, test_true = test(model, A_mat[k], test_loader_all[k], k)\n",
    "        \n",
    "        np.savetxt(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\"_pred.csv\", test_pred.detach().cpu().numpy().reshape(test_pred.shape[0:2]), delimiter=\",\")\n",
    "        np.savetxt(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\"_true.csv\", test_true.detach().cpu().numpy().reshape(test_true.shape[0:2]), delimiter=\",\")\n",
    "\n",
    "        print(\"Training, Test loss: ({:.6f}, {:.6f})\".format(train_losses['train_loss'][-1], test_losses['test_loss'][-1]))\n",
    "\n",
    "        wo_skip_train_history.append(train_losses)\n",
    "        wo_skip_test_results.append(test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN - Subway and Bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation for subway**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2019-09-08 00:00:00'\n",
    "train_end = '2019-09-28 19:00:00'\n",
    "test_start = '2019-09-28 20:00:00'\n",
    "test_end = '2019-10-11 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw/subway_and_bus_station')\n",
    "subway_station = pd.read_csv('subway_station.csv')\n",
    "start_lat = 37.526531\n",
    "start_lng = 127.037807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=subway_station\n",
    "data = data[(data['substn_lat']>start_lat) & (data['substn_lon']>start_lng) & (data['substn_lat']>start_lat) & (data['substn_lon']>start_lng)]\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "    \n",
    "start_coordinate_x = []\n",
    "start_coordinate_y = []\n",
    "end_coordinate_x = []\n",
    "end_coordinate_y = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    start_coordinate_x_tmp = geodesic((start_lat, start_lng), (start_lat, data['substn_lon'][i])).meters\n",
    "    start_coordinate_y_tmp = geodesic((start_lat, start_lng), (data['substn_lat'][i], start_lng)).meters\n",
    "\n",
    "    end_coordinate_x_tmp = geodesic((start_lat, start_lng), (start_lat, data['substn_lon'][i])).meters\n",
    "    end_coordinate_y_tmp = geodesic((start_lat, start_lng), (data['substn_lat'][i], start_lng)).meters\n",
    "\n",
    "    start_coordinate_x.append(start_coordinate_x_tmp)\n",
    "    start_coordinate_y.append(start_coordinate_y_tmp)\n",
    "\n",
    "    end_coordinate_x.append(end_coordinate_x_tmp)\n",
    "    end_coordinate_y.append(end_coordinate_y_tmp)\n",
    "\n",
    "data['substn_x'] = start_coordinate_x\n",
    "data['substn_y'] = start_coordinate_y\n",
    "data['substn_x'] = end_coordinate_x\n",
    "data['substn_y'] = end_coordinate_y\n",
    "    \n",
    "subway_xy = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_xy = subway_xy[(subway_xy['substn_x']>500)&(subway_xy['substn_x']<5000)&(subway_xy['substn_y']>500)&(subway_xy['substn_y']<4000)]\n",
    "subway_xy_uniq = subway_xy.drop([108,110], axis=0).drop(['substn_lon','substn_lat'], axis=1).reset_index(drop=True).reset_index(drop=True)\n",
    "subway_xy_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stn = len(subway_xy_uniq)\n",
    "A_mat_subway = [[0 for x in range(num_stn)] for y in range(num_stn)]\n",
    "\n",
    "for i in range(num_stn):\n",
    "    for j in range(num_stn):\n",
    "        A_mat_subway[i][j] = (((subway_xy_uniq.iloc[i][2]-subway_xy_uniq.iloc[j][2])**2+(subway_xy_uniq.iloc[i][3]-subway_xy_uniq.iloc[j][3])**2)**0.5)/1000.\n",
    "A_mat_subway = np.asarray(A_mat_subway)\n",
    "A_mat_subway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation for bus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw/subway_and_bus_station')\n",
    "bus_station = pd.read_csv('bus_station_deer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bus_station\n",
    "data = data[(data['busstn_lat']>start_lat) & (data['busstn_lon']>start_lng) & (data['busstn_lat']>start_lat) & (data['busstn_lon']>start_lng)]\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "    \n",
    "start_coordinate_x = []\n",
    "start_coordinate_y = []\n",
    "end_coordinate_x = []\n",
    "end_coordinate_y = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    start_coordinate_x_tmp = geodesic((start_lat, start_lng), (start_lat, data['busstn_lon'][i])).meters\n",
    "    start_coordinate_y_tmp = geodesic((start_lat, start_lng), (data['busstn_lat'][i], start_lng)).meters\n",
    "\n",
    "    end_coordinate_x_tmp = geodesic((start_lat, start_lng), (start_lat, data['busstn_lon'][i])).meters\n",
    "    end_coordinate_y_tmp = geodesic((start_lat, start_lng), (data['busstn_lat'][i], start_lng)).meters\n",
    "\n",
    "    start_coordinate_x.append(start_coordinate_x_tmp)\n",
    "    start_coordinate_y.append(start_coordinate_y_tmp)\n",
    "\n",
    "    end_coordinate_x.append(end_coordinate_x_tmp)\n",
    "    end_coordinate_y.append(end_coordinate_y_tmp)\n",
    "\n",
    "data['busstn_x'] = start_coordinate_x\n",
    "data['busstn_y'] = start_coordinate_y\n",
    "data['busstn_x'] = end_coordinate_x\n",
    "data['busstn_y'] = end_coordinate_y\n",
    "    \n",
    "bus_xy = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_xy = bus_xy[(bus_xy['busstn_x']>500)&(bus_xy['busstn_x']<5000)&(bus_xy['busstn_y']>500)&(bus_xy['busstn_y']<4000)]\n",
    "bus_xy_uniq = bus_xy.drop_duplicates(subset='busstn_name').drop(['busstn_lon','busstn_lat'], axis=1).reset_index(drop=True)\n",
    "bus_xy_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stn = len(bus_xy_uniq)\n",
    "A_mat_bus = [[0 for x in range(num_stn)] for y in range(num_stn)]\n",
    "\n",
    "for i in range(num_stn):\n",
    "    for j in range(num_stn):\n",
    "        A_mat_bus[i][j] = (((bus_xy_uniq.iloc[i][2]-bus_xy_uniq.iloc[j][2])**2+(bus_xy_uniq.iloc[i][3]-bus_xy_uniq.iloc[j][3])**2)**0.5)/1000.\n",
    "A_mat_bus = np.asarray(A_mat_bus)\n",
    "A_mat_bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**í•„ìš”í•œ í•¨ìˆ˜ ì •ì˜í•˜ê¸°**\n",
    "* get_normalized_adj() : A_waveë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_adj(A):\n",
    "    A = A + np.diag(np.ones(A.shape[0], dtype=np.float32))\n",
    "    D = np.array(np.sum(A, axis=1)).reshape((-1,))\n",
    "    D[D <= 10e-5] = 10e-5\n",
    "    diag = np.reciprocal(np.sqrt(D))\n",
    "    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A), diag.reshape((1, -1)))\n",
    "    return A_wave\n",
    "A_mat_subway = torch.FloatTensor(get_normalized_adj(A_mat_subway))\n",
    "A_mat_bus = torch.FloatTensor(get_normalized_adj(A_mat_bus))\n",
    "\n",
    "\n",
    "A_mat = {}\n",
    "\n",
    "k=12\n",
    "A_mat[k] = A_mat_subway\n",
    "k=204\n",
    "A_mat[k] = A_mat_bus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prepare dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'02_data_preprocessing')\n",
    "od_mat_nearestpt = pd.read_csv('14_od_mat_nearestpt.csv')\n",
    "miss_o_nearestpt = pd.read_csv('15_miss_o_nearestpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearestpt = np.concatenate((od_mat_nearestpt[['use_start_at_round', 'near_subway_idx', 'near_bus_idx']],miss_o_nearestpt[['walk_fore_at_round', 'near_subway_idx', 'near_bus_idx']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split train and test\n",
    "- time demand array ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2019-09-08 00:00:00'\n",
    "# train_start = datetime.strptime('2019-09-08 00:00:00','%m-%d-%y ')\n",
    "train_end = '2019-09-28 19:00:00'\n",
    "test_start = '2019-09-28 20:00:00'\n",
    "test_end = '2019-10-11 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=12\n",
    "date_time_range = pd.date_range(start=train_start, end=train_end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "train_demand_subway = pd.DataFrame(0, index = date_time_range, columns=range(k))\n",
    "\n",
    "for i in range(len(nearestpt)):\n",
    "    for j in range(len(train_demand_subway)):\n",
    "        if nearestpt[i][0] == train_demand_subway.index[j]:\n",
    "            train_demand_subway.iloc[j,nearestpt[i][1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=12\n",
    "date_time_range = pd.date_range(start=test_start, end=test_end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "test_demand_subway = pd.DataFrame(0, index = date_time_range, columns=range(k))\n",
    "\n",
    "for i in range(len(nearestpt)):\n",
    "    for j in range(len(test_demand_subway)):\n",
    "        if nearestpt[i][0] == test_demand_subway.index[j]:\n",
    "            test_demand_subway.iloc[j,nearestpt[i][1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=204\n",
    "date_time_range = pd.date_range(start=train_start, end=train_end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "train_demand_bus = pd.DataFrame(0, index = date_time_range, columns=range(k))\n",
    "\n",
    "for i in range(len(nearestpt)):\n",
    "    for j in range(len(train_demand_bus)):\n",
    "        if nearestpt[i][0] == train_demand_bus.index[j]:\n",
    "            train_demand_bus.iloc[j,nearestpt[i][2]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=204\n",
    "date_time_range = pd.date_range(start=test_start, end=test_end, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "test_demand_bus = pd.DataFrame(0, index = date_time_range, columns=range(k))\n",
    "\n",
    "for i in range(len(nearestpt)):\n",
    "    for j in range(len(test_demand_bus)):\n",
    "        if nearestpt[i][0] == test_demand_bus.index[j]:\n",
    "            test_demand_bus.iloc[j,nearestpt[i][2]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**put into DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = [12, 204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_demand = {}\n",
    "test_time_demand = {}\n",
    "\n",
    "k=12\n",
    "train_time_demand[k] = np.asarray(train_demand_subway).transpose()\n",
    "test_time_demand[k] = np.asarray(test_demand_subway).transpose()\n",
    "k=204\n",
    "train_time_demand[k] = np.asarray(train_demand_bus).transpose()\n",
    "test_time_demand[k] = np.asarray(test_demand_bus).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "\n",
    "holiday = pd.read_csv('holiday_deer.csv')\n",
    "weather = pd.read_csv('weather_deer.csv')\n",
    "weather = weather.iloc[:793]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create node_feature matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_dataset(demand, holiday, weather, k):\n",
    "    time_len = len(demand[k][0])\n",
    "    time_slice = np.zeros((time_len-5, k, (5+2)*5-2))\n",
    "    for idx in range(time_len-5):\n",
    "        demand_time_slice = np.zeros((k, 5+2))\n",
    "        for c in range(k):\n",
    "            demand_k_min = demand[k][c,idx:idx+5].min()\n",
    "            demand_k_max = demand[k][c,idx:idx+5].max()\n",
    "\n",
    "            if (demand_k_max-demand_k_min)==0:\n",
    "                demand_k_normalize = demand[k][c,idx:idx+5]\n",
    "            else:\n",
    "                demand_k_normalize = (demand[k][c,idx:idx+5]-demand_k_min)/(demand_k_max-demand_k_min)\n",
    "\n",
    "            demand_time_slice[c] = np.append(demand_k_normalize, [demand_k_min, demand_k_max])\n",
    "        \n",
    "        holiday_normalize = holiday['holidays'].iloc[idx:idx+5]\n",
    "        holiday_time_slice = np.tile(holiday_normalize, (k, 1))\n",
    "        \n",
    "        temp_min = weather['temperature'].iloc[idx:idx+5].min()\n",
    "        temp_max = weather['temperature'].iloc[idx:idx+5].max()\n",
    "        if (temp_max-temp_min)==0 and temp_min==0:\n",
    "            temp_normalize = weather['temperature'].iloc[idx:idx+5]\n",
    "        elif (temp_max-temp_min)==0 and temp_min!=0:\n",
    "            temp_normalize = weather['temperature'].iloc[idx:idx+5]/temp_min\n",
    "        else:\n",
    "            temp_normalize = (weather['temperature'].iloc[idx:idx+5]-temp_min)/(temp_max-temp_min)\n",
    "#         temp_normalize = (weather['temperature'].iloc[idx:idx+5]-temp_min)/(temp_max-temp_min)\n",
    "        temp_time_slice = np.tile(np.append(temp_normalize, [temp_min, temp_max]), (k, 1))\n",
    "                          \n",
    "        rain_min = weather['rainfall'].iloc[idx:idx+5].min()\n",
    "        rain_max = weather['rainfall'].iloc[idx:idx+5].max()\n",
    "        if (rain_max-rain_min)==0 and rain_min==0:\n",
    "            rain_normalize = weather['rainfall'].iloc[idx:idx+5]\n",
    "        elif (rain_max-rain_min)==0 and rain_min!=0:\n",
    "            rain_normalize = weather['rainfall'].iloc[idx:idx+5]/rain_min\n",
    "        else:\n",
    "            rain_normalize = (weather['rainfall'].iloc[idx:idx+5]-rain_min)/(rain_max-rain_min)\n",
    "        rain_time_slice = np.tile(np.append(rain_normalize, [rain_min, rain_max]), (k, 1))                   \n",
    "\n",
    "        wind_min = weather['windspeed'].iloc[idx:idx+5].min()\n",
    "        wind_max = weather['windspeed'].iloc[idx:idx+5].max()\n",
    "        if (wind_max-wind_min)==0 and wind_min==0:\n",
    "            wind_normalize = weather['windspeed'].iloc[idx:idx+5]\n",
    "        elif (wind_max-wind_min)==0 and wind_min!=0:\n",
    "            wind_normalize = weather['windspeed'].iloc[idx:idx+5]/wind_min\n",
    "        else:\n",
    "            wind_normalize = (weather['windspeed'].iloc[idx:idx+5]-wind_min)/(wind_max-wind_min)\n",
    "        wind_time_slice = np.tile(np.append(wind_normalize, [wind_min, wind_max]), (k, 1))         \n",
    "\n",
    "        time_slice[idx] = np.concatenate((demand_time_slice, holiday_time_slice, temp_time_slice, rain_time_slice, wind_time_slice), axis=1)\n",
    "    return time_slice\n",
    "\n",
    "def create_y_dataset(data, k):\n",
    "    time_len = len(data[k][0])\n",
    "    time_slice = np.zeros((time_len-5, k))\n",
    "    for idx in range(time_len-5):\n",
    "        time_slice[idx] = data[k][:,idx+5]\n",
    "    return time_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_x = {}\n",
    "train_all_y = {}\n",
    "\n",
    "test_all_x = {}\n",
    "test_all_y = {}\n",
    "\n",
    "for k in k_list:\n",
    "    train_all_x[k] = create_x_dataset(train_time_demand, holiday.iloc[0:500], weather.iloc[0:500], k)\n",
    "    train_all_y[k] = create_y_dataset(train_time_demand, k)\n",
    "\n",
    "    test_all_x[k] = create_x_dataset(test_time_demand, holiday.iloc[500:], weather.iloc[500:], k)\n",
    "    test_all_y[k] = create_y_dataset(test_time_demand, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "with open('test_all_y_pt.p', 'wb') as f:\n",
    "    pickle.dump(test_all_y, f)\n",
    "with open('test_time_demand_pt.p', 'wb') as f:\n",
    "    pickle.dump(test_time_demand, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_all = {}\n",
    "train_loader_all = {}\n",
    "test_ds_all = {}\n",
    "test_loader_all = {}\n",
    "\n",
    "for k in k_list:\n",
    "    train_all_x[k] = torch.FloatTensor(train_all_x[k])\n",
    "    train_all_y[k] = torch.FloatTensor(train_all_y[k])\n",
    "    test_all_x[k] = torch.FloatTensor(test_all_x[k])\n",
    "    test_all_y[k] = torch.FloatTensor(test_all_y[k])\n",
    "    \n",
    "    train_ds_all[k] = TensorDataset(train_all_x[k], train_all_y[k])\n",
    "    train_loader_all[k] = DataLoader(train_ds_all[k], batch_size=64, shuffle=False)\n",
    "    \n",
    "    test_ds_all[k] = TensorDataset(test_all_x[k], test_all_y[k])\n",
    "    test_loader_all[k] = DataLoader(test_ds_all[k], batch_size=288, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "with open('train_ds_all_pt.p', 'wb') as f:\n",
    "    pickle.dump(train_ds_all, f)\n",
    "with open('train_loader_all_pt.p', 'wb') as f:\n",
    "    pickle.dump(train_loader_all, f)\n",
    "with open('test_ds_all_pt.p', 'wb') as f:\n",
    "    pickle.dump(test_ds_all, f)\n",
    "with open('test_loader_all_pt.p', 'wb') as f:\n",
    "    pickle.dump(test_loader_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCN Formulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'01_data_raw')\n",
    "with open('train_ds_all_pt.p', 'rb') as f:\n",
    "    train_ds_all = pickle.load(f)\n",
    "with open('train_loader_all_pt.p', 'rb') as f:\n",
    "    train_loader_all = pickle.load(f)\n",
    "with open('test_ds_all_pt.p', 'rb') as f:\n",
    "    test_ds_all = pickle.load(f)\n",
    "with open('test_loader_all_pt.p', 'rb') as f:\n",
    "    test_loader_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in test_loader_all[k]:\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.0, use_activation=True, bias=False,\n",
    "                 use_skip_connection=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.use_activation = use_activation\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        # PyTorch initialization (see docs)\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, X):\n",
    "        node_features, adj_matrix = X\n",
    "        \n",
    "        x = torch.einsum('ijk,km->ijm', node_features.cpu(), self.weight.cpu()).cuda()\n",
    "        x = torch.einsum('ii,jik->jik', adj_matrix.cpu(), x.cpu()).cuda()\n",
    "\n",
    "        # Add bias if necessary\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "\n",
    "        # Apply activation\n",
    "        if self.use_activation:\n",
    "            x = F.relu(x)\n",
    "\n",
    "        if self.use_skip_connection:\n",
    "            x += node_features\n",
    "\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return [x, adj_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gcn(nn.Module):\n",
    "    def __init__(self, nfeatures, nclasses, hidden_size=16, nhidden_layers=0, dropout=0.0,\n",
    "                 use_skip_connection=False):\n",
    "        super(Gcn, self).__init__()\n",
    "\n",
    "        self.use_skip_connection = use_skip_connection\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nclasses = nclasses\n",
    "        self.nfeatures = nfeatures\n",
    "        self.nhidden_layers = nhidden_layers\n",
    "\n",
    "        self.input_layer = GraphConvolution(nfeatures, hidden_size, self.dropout)\n",
    "\n",
    "        # do not use dropout in hidden layers\n",
    "        self.hidden_layers = self.create_conv_sequence(self.nhidden_layers, self.hidden_size,\n",
    "                                                       use_skip_connection=self.use_skip_connection)\n",
    "        self.output_layer = GraphConvolution(hidden_size, nclasses, self.dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_conv_sequence(num_of_layers, hidden_size, use_skip_connection=False):\n",
    "        seq_conv = nn.Sequential()\n",
    "        for i in range(num_of_layers):\n",
    "            seq_conv.add_module(\"Conv \" + str(i),\n",
    "                                GraphConvolution(hidden_size,\n",
    "                                                 hidden_size,\n",
    "                                                 use_skip_connection=use_skip_connection))\n",
    "        return seq_conv\n",
    "\n",
    "    def forward(self, features, adj_matrix):\n",
    "        x = self.input_layer([features, adj_matrix])\n",
    "        x = self.hidden_layers(x)\n",
    "        output = self.output_layer(x)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train processê°€ ì´ë£¨ì–´ì§€ë„ë¡ í•˜ëŠ” í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, A_mat, loader_all, optimizer, epochs, k, n_layers, dropout, lr, step_size, gamma):\n",
    "\n",
    "    model.train()\n",
    "    loss = nn.MSELoss()\n",
    "    \n",
    "    train_history = {\"train_loss\": []}\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        for train_x_data, train_y_data in loader_all:\n",
    "            train_x_data.cuda()\n",
    "            train_y_data.cuda()\n",
    "            output = model(train_x_data, A_mat)\n",
    "            output = output.squeeze()\n",
    "            loss_train = loss(output.cuda(), train_y_data.cuda()) \n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "        # save current loss\n",
    "        scheduler.step()\n",
    "        train_history[\"train_loss\"].append(loss_train.item())\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(\"epochs: {}, loss: {}\".format(i, loss_train))\n",
    "        \n",
    "        # used to check lr of optimizer\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             print('{}th epoch, the state of scheduler is {}'.format(i, param_group['lr']))\n",
    "\n",
    "#     plt.plot(train_history['train_loss'], label='Train History')\n",
    "#     plt.legend()\n",
    "#     plt.savefig(\"{}_k_{}_hl_{}_do_{}_lr_{}_ss_{}_g_{}_loss.png\".format(k, n_layers, dropout, lr, step_size, gamma, train_history['train_loss'][-1]))\n",
    "#     plt.show()\n",
    "        \n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test processê°€ ì´ë£¨ì–´ì§€ë„ë¡ í•˜ëŠ” í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, A_mat, loader_all, k):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        test_history = {\"test_loss\": []}\n",
    "\n",
    "        for test_x_data, test_y_data in loader_all:\n",
    "            test_x_data.cuda()\n",
    "            test_y_data.cuda()\n",
    "            output = model(test_x_data, A_mat)\n",
    "            output = output.squeeze()\n",
    "            loss_test = loss(output.cuda(), test_y_data.cuda())\n",
    "        # save current loss\n",
    "        test_history[\"test_loss\"].append(loss_test.item())\n",
    "\n",
    "    return test_history, output, test_y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter ì„¤ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "hidden_size = 32\n",
    "dropout = 0.0\n",
    "\n",
    "n_layers = 4\n",
    "n_iteration = 10\n",
    "\n",
    "# Train parameters (Adam optimizer is used)\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "step_size = 150\n",
    "gamma = 0.2\n",
    "\n",
    "# setting filename\n",
    "plt_date = \"0728 01 gcn-pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCN formulating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(user_directory+'/04_result_analysis/')\n",
    "\n",
    "# Without skip connection\n",
    "\n",
    "wo_skip_train_history = []\n",
    "wo_skip_test_results = []\n",
    "\n",
    "for k in k_list:\n",
    "    for iteration in range(n_iteration):\n",
    "        print(\"==============================\")\n",
    "        print(\"number of cluster:\", k)\n",
    "        print(\"number of iteration:\", iteration)\n",
    "        model = Gcn(nfeatures=33,\n",
    "                    nhidden_layers=n_layers, \n",
    "                    hidden_size= hidden_size,\n",
    "                    nclasses= 1,\n",
    "                    dropout=dropout,\n",
    "                    use_skip_connection=False)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            A_mat[k] = A_mat[k].cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        plt_name = \" k=\"+str(k)+\" \"\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "        train_losses = train(model, A_mat[k], train_loader_all[k], optimizer, epochs, k, n_layers, dropout, lr, step_size, gamma)\n",
    "        \n",
    "        matplotlib.rcdefaults()\n",
    "        plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "        plt.rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('training and validation loss'+plt_name)\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim(0, 40)\n",
    "        plt.plot(train_losses['train_loss'], label='Train History')\n",
    "        plt.legend()\n",
    "        plt.savefig(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\".png\", quality=100, optimize=True, progressive=True)\n",
    "        \n",
    "        test_losses, test_pred, test_true = test(model, A_mat[k], test_loader_all[k], k)\n",
    "        \n",
    "        np.savetxt(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\"_pred.csv\", test_pred.detach().cpu().numpy().reshape(test_pred.shape[0:2]), delimiter=\",\")\n",
    "        np.savetxt(plt_date+plt_name+\"_\"+str(iteration)+\"_\"+str(train_losses['train_loss'][-1])+\"_true.csv\", test_true.detach().cpu().numpy().reshape(test_true.shape[0:2]), delimiter=\",\")\n",
    "\n",
    "        print(\"Training, Test loss: ({:.6f}, {:.6f})\".format(train_losses['train_loss'][-1], test_losses['test_loss'][-1]))\n",
    "\n",
    "        wo_skip_train_history.append(train_losses)\n",
    "        wo_skip_test_results.append(test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results and Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Structure**\n",
    "\n",
    "* Grid size\n",
    " * 14*18 (252ê°œ)\n",
    "\n",
    "* LSTM\n",
    " * Grid structureë¥¼ í™œìš©í•˜ì—¬ ë¹„êµ\n",
    "\n",
    "* GCN\n",
    " * GridëŠ” ëª¨ë¸ì´ ìˆ˜ë ´í•˜ì§€ ì•Šì•„ì„œ í¬í•¨í•˜ì§€ ëª»í•¨.\n",
    "\n",
    "| Number of nodes | Historical Average | LSTM with Grid | GCN with Clluster |\n",
    "|---|:---:|---:|\n",
    "|252| 0.533 | 1.271 | 0.790 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Graph Structure**\n",
    "\n",
    "* Cluster\n",
    " * Kê°€ íŠ¹ì • ì§€ì ì—ì„œë§Œ HAë³´ë‹¤ ì¢‹ìŒ.\n",
    "\n",
    "![cluster](01 figures/figure24.png)\n",
    "\n",
    "* Subway\n",
    " * K=12\n",
    "\n",
    "* Bus\n",
    " * K=204\n",
    "\n",
    "| | Number of nodes | Historical Average | GCN |\n",
    "|---|---|:---:|---:|\n",
    "|Subway| 12 | 39.356 | 54.933 |\n",
    "|Bus | 204 | 0.834 | 1.828 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Proposed novel graph definition for free-floating shared personal mobilities.\n",
    " * For implementing novel deep learning approach (i.e. GCN)\n",
    "\n",
    "* Considered potential unmet demand in the data\n",
    " * If not, the historical demand data would have been underestimated\n",
    " * Unmet demandë¥¼ ê³ ë ¤í•œ ìˆ˜ìš”ì˜ˆì¸¡ìœ¼ë¡œ ì¶”í›„ ì¬ë°°ì¹˜ì—ì„œ í™œìš©ë  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤.\n",
    " * ì´ë¥¼ í†µí•´ unmet demandë¥¼ ì¤„ì¼ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë¨.\n",
    "\n",
    "* ê°™ì€ cluster ìˆ˜ì—ì„œ LSTMë³´ë‹¤ GCNì´ ë” ì˜ë§ëŠ”ë‹¤.\n",
    "\n",
    "* GCNì€ íŠ¹ì • cluster ìˆ˜ì—ì„œë§Œ HAë³´ë‹¤ ì˜ë§ëŠ”ë‹¤.\n",
    "\n",
    "* Cluster ìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë©´ì„œ ë°ì´í„°ê°€ sparseí•´ì§ˆìˆ˜ë¡ ì˜ˆì¸¡í•˜ê¸° ì–´ë µë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations and Future works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ì§€ì—­ë§ˆë‹¤ ê¸°ì¤€ì´ ë‹¤ë¥´ê²Œ ì ìš©ë  ìˆ˜ ìˆë‹¤.\n",
    " * ì§€ë¦¬ì  íŠ¹ì„±ì´ë‚˜ ì‚¬ëŒë“¤ì˜ ì´ìš© íŠ¹ì„±ì— ë”°ë¼ ì˜ˆì¸¡ë ¥ì´ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚œë‹¤.\n",
    "\n",
    "* í”¼í¬ì— ëŒ€í•œ ë¶„ì„ì´ ë¶€ì¡±í–ˆë‹¤.\n",
    " * PeakRMSE and PeakMAE\n",
    "\n",
    "* ì ì ˆí•œ Adjacency matrixì— ëŒ€í•œ ë‚´ìš©\n",
    " * ì¶”í›„ ì—°êµ¬ í•„ìš”\n",
    "\n",
    "* Feature extractionì´ í•„ìš”í•˜ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lin, L., He, Z., & Peeta, S. (2018). Predicting station-level hourly demand in a large-scale bike-sharing network: A graph convolutional neural network approach. Transportation Research Part C: Emerging Technologies, 97(November), 258â€“276. https://doi.org/10.1016/j.trc.2018.10.011\n",
    "2. Kim, T. S., Lee, W. K., & Sohn, S. Y. (2019). Graph convolutional network approach applied to predict hourly bike-sharing demands considering spatial, temporal, and global effects. PLoS ONE, 14(9), 1â€“16. https://doi.org/10.1371/journal.pone.0220782\n",
    "3. Kipf, T. N., & Welling, M. (2019). Semi-supervised classification with graph convolutional networks. 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, 1â€“14.\n",
    "4. https://miro.medium.com/max/1070/1*ivtGccpkSXEDVwf9mgyaNg.png\n",
    "5. Yu, B., Yin, H., & Zhu, Z. (2018). Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting. IJCAI International Joint Conference on Artificial Intelligence, 2018-July, 3634â€“3640. https://doi.org/10.24963/ijcai.2018/505\n",
    "6. Ham, S. W., Cho, J. H., & Kim, D. K. (2020). Spatiotemporal Demand Prediction Model for Personal Mobility with Latent Feature and Deep Learning: Considering its Intrinsic Features. In Preparation.\n",
    "7. Cho, J. H., Ham, S. W., & Kim, D. K. (2020). Enhancing the Accuracy of Peak Hourly Demand in Bike-Sharing Systems using a Graph Convolutional Network with Public Transit Usage Data. Under Review.\n",
    "8. Yang, Y., A. Heppenstall, A. Turner, and A. Comber. (2019). A Spatiotemporal and Graph-Based Analysis of Dockless Bike Sharing Patterns to Understand Urban Flows over the Last Mile. Computers, Environment and Urban Systems, Vol. 77, No. July, p. 101361. https://doi.org/10.1016/j.compenvurbsys.2019.101361."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical and technical support of Deer Corporation is truly appreciated. The assist was provided both in data and the whole process of research.\n",
    "\n",
    "This research was results of a study on the \"HPC Support\" Project, 1 supported by the 'Ministry of Science and ICT' and NIPA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors confirm contribution to the paper as follows: study conception and design: S.-W. Ham, J.-H. Cho; data collection: S.-W. Ham; analysis and interpretation of results: S.-W. Ham, J.-H. Cho; draft manuscript preparation: S.-W. Ham, J.-H. Cho.\n",
    "\n",
    "All authors reviewed the results and approved the final version of the manuscript."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
